{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the Auto Encoder ?\n",
    "\n",
    "- Là một mạng thần kinh cố gắng xây dựng lại đầu vào của nó, thường dùng cho các bài toán học không giám sát. \n",
    "\n",
    "- Mục đích: tìm hiểu cách biểu diễn (mã hóa) cho một tập hợp dữ liệu, thường là để giảm kích thước , bằng cách đào tạo mạng để bỏ qua tín hiệu nhiễu nhiễu.\n",
    "\n",
    "- ứng dụng: \n",
    "\n",
    "    + Bài toán segmentation image\n",
    "    + Bài toán giảm kích thước dữ liêu.\n",
    "    + Phát hiện bất thường \n",
    "    + Dịch máy...\n",
    "\n",
    "Ngoài ra còn các biến thể khác nữa cũng sử dụng auto encoder?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to work?\n",
    "\n",
    "Bài toán: Tìm một hình ảnh giống với các hình ảnh khác trong cơ sở dữ liệu.\n",
    "\n",
    "Phương pháp giải quyết:\n",
    "\n",
    "+ Chúng ta không có nhãn của tất cả các ảnh trên và thứ chúng ta có là tập các hình ảnh vậy thì làm sao để có thể so sánh các ảnh với nhau.\n",
    "\n",
    "+ Các hữu hiệu nhất là tìm đặc trưng hình ảnh thông qua các thuật toán rút trích đặc trưng như HOG, SIFT, SUFT, hay ảnh biển... Sau đó áp dụng việc tìm kiếm thông qua khoảng cách Euclid của các đặc trưng đó như k-nnn. Nhưng chúng có thực sự hiệu quả. Chúng sẽ hiệu quả trong một vài trường hợp, cấu trúc ảnh khác nhau và độ phức tạp dữ liệu không cao.\n",
    "\n",
    "Điều tiên quyết là làm sao chúng ta có một trình trích xuất tính năng mạnh mẽ. \n",
    "\n",
    "Trong vài năm gần đây sự gia tăng của dữ liệu khiến cho deeplearning là một trong nhưng phương pháp thay thế hoàn hảo cho các thuật toán truyền thống. Cụ thể hơn trong bài toán trên chúng ta sẽ áp dụng một mô hình mạng AE.\n",
    "\n",
    "Vậy áp dụng như thế nào???\n",
    "\n",
    "Đầu ra mong muốn của chúng ta là vector đặc trưng của đàu vào đó, nhưng chúng ta lại không nhãn của từng ảnh, nếu build một cấu trúc CNN theo cách thông thường bài toán classification sẽ không thể tối ưu được. \n",
    "\n",
    "Vậy bài toán học không giám sát được sử dụng trong TH này.\n",
    "\n",
    "Mô tả toán học AE gồm 2 phần:\n",
    "\n",
    "Phần Encode\n",
    "* Tập đầu vào: X\n",
    "* Hàm phi tuyến rút trích đặc trưng: f(x) ( có thể là các lớp Conv, Maxpool ..)\n",
    "* Đầu ra sau hàm phi tuyến là: $ X_{encode} $ = f(X)\n",
    "\n",
    "\n",
    "Phần Decode:\n",
    "AE là một mạng cố gắng xây dựng lại đầu vào vậy chúng ta cố gắng biến đổi $ X_{encode} $-> X\n",
    "\n",
    "Tương tự vậy chúng ta phải áp dụng một, nhiều hàm phi tuyến g(x) \n",
    "\n",
    "Khi đó X_pred = g($ X_{encode} $) \n",
    "\n",
    "Mục tiêu là tối ưu sao X_pred sao cho giống X nhất có thể. Thì khi đó $ X_{encode} $ đã mã hóa càng trích xác dữ liệu\n",
    "\n",
    "Hàm loss có thể là: loss = $(X_{pred} - X) ^ 2 $\n",
    "\n",
    "Như vậy nếu tối ưu được loss kia thì chúng ta hoàn toàn trích được đặc trưng của đầu vào mà không cần nhãn. \n",
    "\n",
    "\n",
    "#### Cấu trúc mạng \n",
    "![](https://miro.medium.com/max/3148/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phần Encode:\n",
    "\n",
    "* Là một mạng CNN được thiết kế sử dụng lớp Conv, Dilated Convolutions, cùng lớp MaxPool, Dropout...\n",
    "\n",
    "Mô tả Dilated Conv:\n",
    "\n",
    "![](https://miro.medium.com/max/790/1*SVkgHoFoiMZkjy54zM_SUw.gif)\n",
    "\n",
    "Tìm hiểu về các loại Conv: https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d\n",
    "\n",
    "Phần Decode:\n",
    "\n",
    "* Là một mạng CNN nhưng sử dụng: Transposed Convolutions \n",
    "\n",
    "Mô tả về: Transposed Convolutions \n",
    "\n",
    "![](https://miro.medium.com/max/790/1*Lpn4nag_KRMfGkx1k6bV-g.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Các loại Auto-Encoder\n",
    "\n",
    "1. Sparse Autoencoders\n",
    "\n",
    "![](https://miro.medium.com/max/854/1*lLRsOzq78DOGAjlkS0UetQ.png)\n",
    "\n",
    "Đặc điểm:\n",
    "\n",
    "+ Các nút ẩn lớn hơn các nút đầu vào. Họ vẫn có thể khám phá các tính năng quan trọng từ dữ liệu.\n",
    "+ Ràng buộc thư thớt ngăn chặn sao chép dữ liệu từ đầu vào thành các lớp ẩn\n",
    "+ Bộ tự động thưa thớt có hình phạt thưa thớt, (h), giá trị gần bằng 0 nhưng không bằng không. Hình phạt thưa thớt được áp dụng cho lớp ẩn bên cạnh lỗi tái cấu trúc. Điều này ngăn ngừa quá mức:\n",
    "    \n",
    "    ![](https://miro.medium.com/max/255/1*WpjQpwkHk6FMw4K4dLso7A.png)\n",
    "    \n",
    "+ Bộ điều khiển tự động thưa thớt lấy các giá trị kích hoạt cao nhất trong lớp ẩn và loại bỏ phần còn lại của các nút ẩn. Điều này ngăn các bộ tự động sử dụng tất cả các nút ẩn tại một thời điểm và chỉ buộc một số nút ẩn bị giảm được sử dụng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Denoising Autoencoders (Khử nhiễm)\n",
    "\n",
    "![](https://miro.medium.com/max/1212/1*R05MPVwHnDEJRm6V_dEYpw.png)\n",
    "\n",
    "Đặc điểm:\n",
    "\n",
    "+ Sử dụng trong bài toán khử nhiễm đầu vào. Khi đầu vào được thêm các nhiễm vào cần được làm sạch.\n",
    "+ Denoising Autoencoders tạo ra một bản sao bị hỏng của đầu vào bằng cách đưa ra một số nhiễu. Điều này giúp tránh các bộ tự động sao chép đầu vào vào đầu ra mà không tìm hiểu các tính năng về dữ liệu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Contractive Auto Encoders ( tương phản)\n",
    "\n",
    "![](https://miro.medium.com/max/818/1*Lhdfshqc6HxfFGuIhQyaAw.png)\n",
    "\n",
    "Đặc điểm:\n",
    "\n",
    "+ Xây dựng một cấu trúc mạng mạnh mẽ để không bị ảnh hưởng nhỏ của dữ liệu. \n",
    "\n",
    "+ Thực hiện bằng cách áp dụng thời hạn phạt cho chức năng mất\n",
    "\n",
    "![](https://miro.medium.com/max/240/1*lAl_hrpIP3Tz_w-O1kJ44w.png)\n",
    "\n",
    "\n",
    "Định mức Frobenius của ma trận Jacobian cho lớp ẩn được tính tương ứng với đầu vào. Định mức Frobenius của ma trận Jacobian là tổng bình phương của tất cả các phần tử.\n",
    "\n",
    "4. Stacked Auto Encoders (Xếp chồng)\n",
    "\n",
    "![](https://miro.medium.com/max/1212/1*-kjH9_RsvMtprV_GmHUoMA.png)\n",
    "\n",
    "Đặc điểm:\n",
    "\n",
    "+ Bộ xếp tự động xếp chồng là một mạng nơ-ron với nhiều lớp bộ tự động thưa thớt\n",
    "5. Deep Auto Encoders \n",
    "\n",
    "![](https://miro.medium.com/max/908/1*UpWsxOB-3D1nMYgYUEz2NQ.png)\n",
    "\n",
    "\n",
    "Đặc điểm:\n",
    "+ Deep Autoencoders bao gồm hai mạng niềm tin sâu giống hệt nhau. Một mạng để mã hóa và một mạng khác để giải mã\n",
    "+ Thông thường bộ tự động sâu có 4 đến 5 lớp để mã hóa và 4 đến 5 lớp tiếp theo để giải mã. Chúng tôi sử dụng lớp không giám sát bởi lớp đào tạo trước.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tổng kết\n",
    "\n",
    "- Autoencoder là các mạng thần kinh được đào tạo để xây dựng lại đầu vào ban đầu của chúng.\n",
    "- Autoencoder là một dạng thuật toán trích xuất tính năng.\n",
    "- Bộ tự động có thể được xếp chồng lên nhau.\n",
    "- Đầu ra của bộ mã hóa tự động là lớp giữa, đại diện cho từng điểm dữ liệu.\n",
    "- Chúng ta có thể sử dụng các tính năng được tạo bởi AE trong bất kỳ thuật toán nào khác, ví dụ để phân loại."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đọc thêm:\n",
    "    \n",
    "[1]: https://medium.com/machine-learning-researcher/auto-encoder-d942a29c9807\n",
    "\n",
    "[2]: https://en.wikipedia.org/wiki/Autoencoder\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
