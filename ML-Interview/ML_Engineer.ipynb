{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuNUhs1C1Jou"
      },
      "source": [
        "### Author: Hoang Mau Trung\n",
        "### Position: Machine Learning Engineer\n",
        "### Email: hoangmautrung@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjzrO6HS1ObO"
      },
      "source": [
        "# Part 1: Theoretical Questions\n",
        "\n",
        "## 1.1 Why do GPT models only use the decoder part of the Transformer architecture?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Default Transformer models include 2 parts Encoder and Decoder. The encoder part is given inputs to capture context, it is rich information to support tasks such as machine translate or classification where context affects  final outputs. The decoder part uses to generate outputs.\n",
        "GPT models are designed for tasks that involve generating text based on a given context or prompt (autoregressive text generation) as context is unnecessary. They use decoder part with masked self-attention to create a model, which generates the next tokens based on previously generated tokens. It also makes a simple architecture and training process, allowing the model can be trained on vast amounts of unannotated text data to learn language patterns and structures. \n",
        "\n",
        "\n",
        "## 1.2 Explain the Attention mechanism in Transformer and why it is more effective than traditional RNN architectures. \n",
        "\n",
        "* Generation 1:\n",
        "Let's start with RNN problems in Seq2Seq models:\n",
        "\n",
        "    - No parallel processing: RNNs process input sequentially, leading to slow performance.\n",
        "\n",
        "    - Vanishing gradient problems: This issue hinders the network's ability to learn long-range dependencies.\n",
        "\n",
        "    - Missing context for long input: RNNs struggle to retain information from earlier parts of long sequences.\n",
        "\n",
        "    - Fixed context vector (bottleneck): This becomes problematic for the decoder with long input, reducing performance.\n",
        "\n",
        "    - While LSTM/GRU were designed to reduce vanishing gradients and capture long range context, they still suffer from low performance.\n",
        "\n",
        "* Generation 2: Attention (Keeping base models is LSTM, GRU for encoder and decoder)\n",
        "\n",
        "    **Key concepts**: The traditional approach uses only the last encoder hidden state.  A better approach is to use all encoder hidden states.\n",
        "\n",
        "    - **Attention Bahdanau**: The Bahdanau attention mechanism improves traditional methods by creating a dynamic context vector for each decoding step. This vector is formed by calculating a weighted sum of all the encoder hidden states. These weights are learned during training and allow the model to focus on the most relevant parts of the input sequence when making predictions. \n",
        "\n",
        "    - **Luong Attention**: The mechanism is same idea with Bahdanau but it is difference about how the alignment scores (weights in hidden encoder state) is caculated. Instead of using the decoder's previous hidden state to calculate these scores, Luong Attention directly compares the current decoder hidden state with each of the encoder hidden states. \n",
        "\n",
        "    **Limitation**:\n",
        "    - Use RNN, LSTM, GRU is sequential calculation -> need a new solution to address this problems.\n",
        "\n",
        "* Generation 3: Transformer \n",
        "\n",
        "    - Transformer has removed sequential calculation by self-attention\n",
        "    - Self-Attention is a mechanism that helps the model focus on important parts of the input by calculating the relationship between all the words in the sentence with each other.\n",
        "\n",
        "    - Self-Attention Visualization: Self-attention can be visualized as a process where each word (or input element) acts as both a \"question\" and an \"answer.\"  Each word \"asks\" every other word, \"How relevant are you to me?\"  The other words then \"answer\" by providing a relevance \"score.\"\n",
        "\n",
        "    - Self-Attention Step by step:\n",
        "\n",
        "        + Step 1: Embed words/tokens into same-size vectors.\n",
        "\n",
        "        + Step 2: Multiply each embedding vector by 3 matrices W_Q, W_K, W_V to get Q, K, V, where: \n",
        "\n",
        "            - Q: Query - Represents how an element \"asks\" others.\n",
        "            - K: Key - Represents for key information. Used to calculate relevance to the Query.\n",
        "            - V: Value - Represents the context of each word.\n",
        "\n",
        "        + Step 3: Attention Weight Calculation (Alignment Scores) \n",
        "\n",
        "        The relevance between word (i) and word (j) is computed by the dot product of the Query of word (i) and the Key of word (j):\n",
        "\n",
        "        $$\n",
        "        Score(Q, K) = Q.K^T\n",
        "        $$\n",
        "\n",
        "        + Step 4: Softmax Normalization of Attention Weights\n",
        "\n",
        "        The attention weights are normalized using the Softmax function to ensure that the sum of all weights equals 1:\n",
        "\n",
        "        $$\n",
        "        α_{ij} = softmax( (Q.K^T) / sqrt(d_k) )\n",
        "        $$\n",
        "\n",
        "        + Step 5: Multiplying Weights with Value to Create Attention Output \n",
        "        Sau khi có trọng số, chúng ta nhân với Value để tạo đầu ra:\n",
        "\n",
        "        $$\n",
        "        Attention(Q, K, V) = softmax( (Q.K^T) / sqrt(d_k) ) V\n",
        "        $$\n",
        "\n",
        "    - Multi-Head Self-Attention: Multi-Head Attention uses multiple different \"heads\" to learn diverse relationships within the data instead only one head like as Self-Attention.\n",
        "\n",
        "    $$\n",
        "    MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h).W_O\n",
        "    $$\n",
        "\n",
        "## 1.3 Explain the Chain-of-Thought technique. Analyze the advantages and disadvantages of this Prompt Engineering technique.\n",
        "\n",
        "* Explain CoT technique \n",
        "\n",
        "CoT is a way mimics human processing a complex task, it will break problem into simple step by step to gain final result. It is a way we address a math exercise in high shool. It encourages the model reason through intermediate steps before make final answer. By prompting the model with examples of reasoning chains, it can better generalize to new, complex problems. \n",
        "\n",
        "Example: How many rows in string \"hja34r87392rrr83873rrrr\"\n",
        "\n",
        "Without CoT: LLM will start generate: \n",
        "\n",
        "```Number of 'r' in string 'hja34r87392rrr83873rrrr' is 10.\"```\n",
        "\n",
        "With CoT:\n",
        "\n",
        "```\n",
        "Let's count the number of occurrences of the letter 'r' in the string \"hja34r87392rrr83873rrrr\" step by step.\n",
        "\n",
        "Given string:\n",
        "hja34r87392rrr83873rrrr\n",
        "\n",
        "Step-by-step counting:\n",
        "hja34r87392rrr83873rrrr\n",
        "First 'r' at index 5\n",
        "Second 'r' at index 10\n",
        "Third 'r' at index 11\n",
        "Fourth 'r' at index 12\n",
        "Fifth 'r' at index 17\n",
        "Sixth 'r' at index 18\n",
        "Seventh 'r' at index 19\n",
        "Eighth 'r' at index 20\n",
        "Final count:\n",
        "There are 8 occurrences of the letter 'r' in the given string.\n",
        "```\n",
        "\n",
        "* Advantages and Disadvantages\n",
        "\n",
        "**Advantages**\n",
        "\n",
        "- Enhancing performance of LLM when splitting complex tasks step by step. \n",
        "- Easy control and understanding step-by-step generated by LLM.  \n",
        "- More details about solutions help us build a chain of thought.   \n",
        "- Combining more tools or systems to finish a task (Idea of Agents) \n",
        "\n",
        "**Disadvantages** \n",
        "- Depend on prompts \n",
        "- Consuming more tokens because need to process longer -> more cost, and slower \n",
        "- Over fitting when building a solution from prompt, and always keep one way to process\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2 Practical Exercises\n",
        "\n",
        "## 2.1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "def parse_amount(amount_str: str) -> float:\n",
        "    \"\"\"Convert string amount to float by removing commas.\"\"\"\n",
        "    return float(amount_str.replace(\",\", \"\"))\n",
        "\n",
        "def extract_amounts(text: str) -> List[float]:\n",
        "    \"\"\"Extract all numeric amounts from input string.\n",
        "    \n",
        "    Args:\n",
        "        text: Input string containing amounts (e.g. \"donate1.23buyapple12,390\")\n",
        "    \n",
        "    Returns:\n",
        "        List of extracted amounts as floats\n",
        "    \"\"\"\n",
        "    DIGITS = set(\"0123456789.,\")\n",
        "    amounts = []\n",
        "    current = []\n",
        "    \n",
        "    for char in text:\n",
        "        if char in DIGITS:\n",
        "            current.append(char)\n",
        "        elif current:\n",
        "            amounts.append(\"\".join(current))\n",
        "            current.clear()\n",
        "            \n",
        "    if current:\n",
        "        amounts.append(\"\".join(current))\n",
        "        \n",
        "    return [parse_amount(amt) for amt in amounts]\n",
        "\n",
        "def format_amount(total_amount: float) -> str:\n",
        "\n",
        "    \"\"\"Format total amount int defined format\"\"\"\n",
        "    str_total_amount = str(total_amount)\n",
        "    cent = str_total_amount.split(\".\")[-1]\n",
        "    if len(cent) == 1: str_total_amount += '0'\n",
        "    \n",
        "    dollar = str_total_amount[:-3]\n",
        "    cent = str_total_amount[-3:]\n",
        "\n",
        "    formatted_dollar = \"\"\n",
        "    for i, digit in enumerate(reversed(dollar)):\n",
        "        if i > 0 and i % 3 == 0:\n",
        "            formatted_dollar = \",\" + formatted_dollar\n",
        "        formatted_dollar = digit + formatted_dollar\n",
        "    \n",
        "    result = formatted_dollar + cent \n",
        "\n",
        "    if result.endswith(\"00\"):\n",
        "        result = result[:-3]\n",
        "    \n",
        "    return result \n",
        "\n",
        "def calculate_total(text: str) -> float:\n",
        "    \"\"\"Calculate total of all amounts in input string.\"\"\"\n",
        "    total_amounts = round(sum(extract_amounts(text)),2)\n",
        "    return format_amount(total_amounts)\n",
        "\n",
        "\"\"\"\n",
        "Floating point will make some issues \n",
        "\n",
        "0.01 + 0.05 -> 0.060000000000000005\n",
        "\n",
        "0.01 + 0.06 -> 0.06999999999999999\n",
        "\n",
        "So I use func round in sum value\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------\n",
            "12,391.23\n",
            "------------------------\n",
            "0.03\n",
            "------------------------\n",
            "6.45\n",
            "------------------------\n",
            "10\n",
            "------------------------\n",
            "0.06\n"
          ]
        }
      ],
      "source": [
        "inputs = [\"donate1.23buyapple12,390\", \"aa0.01t0.02\", \"a1b2c3.45\", \"p0.05c9.95\", \"a0.01b0.05\"]\n",
        "for _input in inputs:\n",
        "    print(\"------------------------\")\n",
        "    print(calculate_total(_input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 \n",
        "\n",
        "The company's product currently needs a module to classify medical documents into 10 different types (Patient Records, Prescriptions, Infusion Guidelines, Treatment Protocols, etc.). Given a labeled dataset of pairs (medical document PDF - document type label) \n",
        "\n",
        "Briefly describe a solution that would effectively solve this problem (which model to use, how to train and test, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From this requirements we decide into 2 phases in development pace modeling (select solution about model and train/test cycle.)\n",
        "\n",
        "### Phase 1: Modeling\n",
        "\n",
        "\n",
        "#### Summary Requirements\n",
        "Input:\n",
        "Pair (Document - Label): \n",
        "- Labels are from 10 medical document classes.\n",
        "- Documents are PDFs containing medical text of varying lengths.\n",
        "\n",
        "Output:\n",
        "- Model is able to classify medical documents into 10 different types. \n",
        "- Model should handle variable length inputs effectively\n",
        "\n",
        "### Problems Consider \n",
        "- Document length variation\n",
        "- Medical domain \n",
        "- Technical constrain: speed, acc, env of production \n",
        "\n",
        "### Research and discussion  \n",
        "- Model type: Text/Document Classification \n",
        "- Model Options:\n",
        "    + BERT\n",
        "    + Sentence Transformer\n",
        "    + LongFormer (or other variations optimized for long documents)\n",
        "\n",
        "Many models are designed to handle long-context inputs, but in practice, we often set a fixed input size, such as 512 tokens for BERT or Sentence Transformer, or a larger size for LongFormer.\n",
        "\n",
        "- To improve classification performance on long documents, we split each document into multiple chunks, with chunk sizes determined by the model's input limit. During prediction, we classify each chunk individually and use a voting mechanism—averaging the results across all chunks—to determine the final document classification.\n",
        "\n",
        "Final solution: \n",
        "\n",
        "Documents -> N x chunks -> Model (BERT or ST) -> N x classes -> Voting -> Final Class\n",
        "\n",
        "* chunks = input sizes of model \n",
        "* N depend on length of documents (should be get overlap documents)\n",
        "\n",
        "- Extend idea:\n",
        "+ Combine result with full text search use ELK to better results. \n",
        "\n",
        "### Training Stage \n",
        "\n",
        "* Stage 1: Data preprocess: reader, chunking, formatting, splitting train:test:eval ratio \n",
        "    + Train:Test:Eval = 70:15:15\n",
        "\n",
        "* Stage 2: Training  \n",
        "\n",
        "* Stage 3: Evaluating\n",
        "\n",
        "Metrics:\n",
        "    + Acc \n",
        "    + F1 score (if imbalance data)\n",
        "    + Confusion matrix \n",
        "    + Recall/Precision per class \n",
        "\n",
        "\n",
        "### Phase 2: MLOps: Design system to serving model to production \n",
        "\n",
        "![](https://github.com/ngoctuhan/AI-Starter/blob/master/ML-Interview/SystemDesignDocumentClassification.drawio.png?raw=true)\n",
        "\n",
        "Some notes from design:\n",
        "\n",
        "+ Data Preparation: Documents -> Preprocessing (Reading/Chunking/Splitting) -> Storage (S3 & Database) \n",
        "+ Model Training: Data Retrieval -> Training Model -> Tracking (MLFlow) -> Checkpoint Selection\n",
        "+ Deployment: Best Checkpoint -> Model Transformation (ONNX, TensorRT) -> Containerization (Triton) -> Deployment -> Monitoring\n",
        "\n",
        "+ Trigger CI/CD (Github Action/Jenkins): Automate the deployment process. When a new best checkpoint is identified (e.g., by MLFlow), trigger a CI/CD pipeline to:\n",
        "\n",
        "    + Transform the model.\n",
        "\n",
        "    + Build the container.\n",
        "\n",
        "    + Run tests (unit tests, integration tests).\n",
        "\n",
        "    + Deploy the new model version to serving environment.\n",
        "\n",
        "+ Monitoring: \n",
        "\n",
        "    + System Metrics: CPU usage, memory usage, GPU utilization, request latency, error rates. \n",
        "    + Model Performance Metrics: Track the model's accuracy, precision, recall, etc., in production.\n",
        "    + Data Drift Detection: Compare the distribution of incoming data to the training data distribution. \n",
        "    + Alerting \n",
        "    + Sentry to log bugs of serving services."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
