{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Bag of Words (CBOW) Overview\n",
    "\n",
    "The Continuous Bag of Words (CBOW) model is a neural network-based approach used in natural language processing (NLP) to generate word embeddings. It is part of the Word2Vec family of models introduced by Mikolov et al. in 2013. The primary goal of the CBOW model is to predict a target word given its surrounding context words within a specified window size.\n",
    "\n",
    "#### Concept and Understanding\n",
    "\n",
    "1. **Context and Target Words**:\n",
    "    - **Context Words**: These are the words surrounding the target word within a specified window size. For example, in the sentence \"The sun was setting behind the mountains,\" if the target word is \"was\" and the window size is 2, the context words are [\"The\", \"sun\", \"setting\", \"behind\"].\n",
    "    - **Target Word**: This is the word that the model aims to predict based on the context words.\n",
    "\n",
    "2. **Model Architecture**:\n",
    "    - **Input Layer**: The input layer consists of the context words represented as one-hot encoded vectors.\n",
    "    - **Hidden Layer**: The hidden layer is a dense layer that projects the input vectors into a lower-dimensional space, creating word embeddings.\n",
    "    - **Output Layer**: The output layer is a softmax layer that predicts the probability distribution over the entire vocabulary for the target word.\n",
    "\n",
    "3. **Training Objective**:\n",
    "    - The CBOW model is trained to maximize the probability of predicting the correct target word given the context words. This is achieved by minimizing the cross-entropy loss between the predicted and actual target words.\n",
    "\n",
    "4. **Advantages**:\n",
    "    - **Efficiency**: CBOW is computationally efficient and can be trained on large datasets.\n",
    "    - **Quality of Embeddings**: The embeddings generated by CBOW capture semantic relationships between words, making them useful for various NLP tasks.\n",
    "\n",
    "5. **Applications**:\n",
    "    - **Word Similarity**: Finding similar words based on their embeddings.\n",
    "    - **Text Classification**: Using word embeddings as features for classification tasks.\n",
    "    - **Machine Translation**: Improving translation quality by leveraging word embeddings.\n",
    "\n",
    "Overall, the CBOW model is a powerful tool for generating word embeddings that capture the semantic meaning of words based on their context in a corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build sample example CBOW with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for CBOW model \n",
    "\n",
    "# Example dataset generated from https://chatgpt.com/\n",
    "example_data = [\n",
    "    \"The sun was setting behind the mountains, casting a golden glow over the valley.\",\n",
    "    \"She opened the book and found an old letter tucked between the pages.\",\n",
    "    \"The cat sat on the windowsill, watching the birds outside with keen interest.\",\n",
    "    \"A sudden gust of wind blew the papers off his desk and onto the floor.\",\n",
    "    \"They decided to take a road trip along the coast, stopping at every small town.\",\n",
    "    \"The scientist carefully recorded the experiment's results in her notebook.\",\n",
    "    \"He could hear the distant sound of thunder as dark clouds gathered overhead.\",\n",
    "    \"The bakery on the corner fills the street with the smell of fresh bread every morning.\",\n",
    "    \"She practiced the piano for hours, determined to perfect the piece before the recital.\",\n",
    "    \"A tiny kitten was curled up in a basket by the fireplace, sleeping peacefully.\",\n",
    "    \"The detective studied the clues, trying to connect the dots in the mystery case.\",\n",
    "    \"They watched as the fireworks exploded in brilliant colors across the night sky.\",\n",
    "    \"He found an old photograph of his grandparents when they were young and in love.\",\n",
    "    \"The children built a sandcastle at the beach, decorating it with seashells.\",\n",
    "    \"The mountain trail was steep and rocky, but the view from the top was breathtaking.\",\n",
    "    \"She received a letter from an old friend she hadn't spoken to in years.\",\n",
    "    \"The library was quiet except for the sound of pages turning and pencils scratching.\",\n",
    "    \"The farmer woke up early to tend to his fields before the sun got too hot.\",\n",
    "    \"A soft melody played on the radio as she sipped her coffee and stared out the window.\",\n",
    "    \"The ancient ruins stood as a reminder of a once-great civilization.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "\n",
    "class CBOWDataset:\n",
    "\n",
    "    def __init__(self, data, window_size=2): \n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: list of sentences\n",
    "            window_size: number of words to consider before and after the target word\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.word_to_idx = {}  \n",
    "        self.idx_to_word = {}\n",
    "        self.vocab = set()\n",
    "        self.create_vocab()\n",
    "        self.create_word_to_idx()\n",
    "        self.create_data()\n",
    "        \n",
    "\n",
    "    def create_vocab(self):\n",
    "        for sentence in self.data:\n",
    "            words = sentence.split()\n",
    "            for word in words:\n",
    "                self.vocab.add(word)\n",
    "\n",
    "    def create_word_to_idx(self):\n",
    "        for idx, word in enumerate(self.vocab):\n",
    "            self.word_to_idx[word] = idx\n",
    "            self.idx_to_word[idx] = word\n",
    "\n",
    "    def create_data(self):\n",
    "        self.context_words = []\n",
    "        self.target_word = []\n",
    "\n",
    "        for sentence in self.data:\n",
    "            words = sentence.split()\n",
    "            for i in range(self.window_size, len(words) - self.window_size):\n",
    "                context = [words[j] for j in range(i - self.window_size, i + self.window_size + 1) if j != i]\n",
    "                target = words[i]\n",
    "                self.context_words.append(context)\n",
    "                self.target_word.append(target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context = self.context_words[idx]\n",
    "        target = self.target_word[idx]\n",
    "        context_idx = torch.tensor([self.word_to_idx[word] for word in context])\n",
    "        target_idx = torch.tensor(self.word_to_idx[target])\n",
    "        return context_idx, target_idx\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def save_vocab(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Save vocabulary and related mappings to JSON file\n",
    "        \n",
    "        Args:\n",
    "            path (str): Path to save vocabulary\n",
    "        \"\"\"\n",
    "        # Save to a JSON file\n",
    "        with open(path, \"w\") as json_file:\n",
    "            json.dump(self.word_to_idx, json_file, indent=4) \n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(path: str) -> None:\n",
    "        \"\"\"\n",
    "        Load vocabulary and related mappings from JSON file\n",
    "        \n",
    "        Args:\n",
    "            path (str): Path to load vocabulary\n",
    "        \"\"\"\n",
    "        # Load from a JSON file\n",
    "        with open(path, \"r\") as json_file:\n",
    "            word_to_idx = json.load(json_file)\n",
    "            idx_to_word = {v: k for k, v in word_to_idx.items()}\n",
    "            vocab = set(word_to_idx.keys())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 48,   7,  54, 175]) tensor(99)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "dataset = CBOWDataset(example_data, window_size=2)\n",
    "context_words, target_word = dataset.__getitem__(0)\n",
    "print(context_words, target_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define CBOW model \n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        \"\"\"\n",
    "        Initialize CBOW model\n",
    "        \n",
    "        Args:\n",
    "            vocab_size (int): Size of vocabulary\n",
    "            embedding_dim (int): Dimension of word embeddings\n",
    "        \"\"\"\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        \n",
    "        Args:\n",
    "            inputs (torch.Tensor): Context word indices [batch_size, context_size]\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Probability distribution over vocabulary\n",
    "        \"\"\"\n",
    "        embeds = self.embeddings(inputs)  # [batch_size, context_size, embed_dim]\n",
    "        hidden = torch.mean(embeds, dim=1)  # [batch_size, embed_dim]\n",
    "        output = self.linear(hidden)  # [batch_size, vocab_size]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and parameters\n",
    "vocab_size = dataset.get_vocab_size()\n",
    "embedding_dim = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW(\n",
      "  (embeddings): Embedding(179, 100)\n",
      "  (linear): Linear(in_features=100, out_features=179, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Train CBOW model\n",
    "dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=True)\n",
    "    \n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CBOW(vocab_size, embedding_dim).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 5.3613\n",
      "Epoch 2/100, Loss: 5.2994\n",
      "Epoch 3/100, Loss: 5.2376\n",
      "Epoch 4/100, Loss: 5.1759\n",
      "Epoch 5/100, Loss: 5.1144\n",
      "Epoch 6/100, Loss: 5.0531\n",
      "Epoch 7/100, Loss: 4.9919\n",
      "Epoch 8/100, Loss: 4.9308\n",
      "Epoch 9/100, Loss: 4.8698\n",
      "Epoch 10/100, Loss: 4.8090\n",
      "Epoch 11/100, Loss: 4.7484\n",
      "Epoch 12/100, Loss: 4.6879\n",
      "Epoch 13/100, Loss: 4.6275\n",
      "Epoch 14/100, Loss: 4.5672\n",
      "Epoch 15/100, Loss: 4.5071\n",
      "Epoch 16/100, Loss: 4.4472\n",
      "Epoch 17/100, Loss: 4.3874\n",
      "Epoch 18/100, Loss: 4.3277\n",
      "Epoch 19/100, Loss: 4.2682\n",
      "Epoch 20/100, Loss: 4.2089\n",
      "Epoch 21/100, Loss: 4.1497\n",
      "Epoch 22/100, Loss: 4.0906\n",
      "Epoch 23/100, Loss: 4.0318\n",
      "Epoch 24/100, Loss: 3.9731\n",
      "Epoch 25/100, Loss: 3.9146\n",
      "Epoch 26/100, Loss: 3.8562\n",
      "Epoch 27/100, Loss: 3.7981\n",
      "Epoch 28/100, Loss: 3.7402\n",
      "Epoch 29/100, Loss: 3.6825\n",
      "Epoch 30/100, Loss: 3.6250\n",
      "Epoch 31/100, Loss: 3.5678\n",
      "Epoch 32/100, Loss: 3.5108\n",
      "Epoch 33/100, Loss: 3.4541\n",
      "Epoch 34/100, Loss: 3.3977\n",
      "Epoch 35/100, Loss: 3.3416\n",
      "Epoch 36/100, Loss: 3.2857\n",
      "Epoch 37/100, Loss: 3.2302\n",
      "Epoch 38/100, Loss: 3.1751\n",
      "Epoch 39/100, Loss: 3.1203\n",
      "Epoch 40/100, Loss: 3.0658\n",
      "Epoch 41/100, Loss: 3.0118\n",
      "Epoch 42/100, Loss: 2.9581\n",
      "Epoch 43/100, Loss: 2.9049\n",
      "Epoch 44/100, Loss: 2.8521\n",
      "Epoch 45/100, Loss: 2.7997\n",
      "Epoch 46/100, Loss: 2.7478\n",
      "Epoch 47/100, Loss: 2.6964\n",
      "Epoch 48/100, Loss: 2.6455\n",
      "Epoch 49/100, Loss: 2.5951\n",
      "Epoch 50/100, Loss: 2.5453\n",
      "Epoch 51/100, Loss: 2.4960\n",
      "Epoch 52/100, Loss: 2.4472\n",
      "Epoch 53/100, Loss: 2.3990\n",
      "Epoch 54/100, Loss: 2.3514\n",
      "Epoch 55/100, Loss: 2.3044\n",
      "Epoch 56/100, Loss: 2.2580\n",
      "Epoch 57/100, Loss: 2.2122\n",
      "Epoch 58/100, Loss: 2.1670\n",
      "Epoch 59/100, Loss: 2.1225\n",
      "Epoch 60/100, Loss: 2.0786\n",
      "Epoch 61/100, Loss: 2.0354\n",
      "Epoch 62/100, Loss: 1.9928\n",
      "Epoch 63/100, Loss: 1.9509\n",
      "Epoch 64/100, Loss: 1.9097\n",
      "Epoch 65/100, Loss: 1.8692\n",
      "Epoch 66/100, Loss: 1.8293\n",
      "Epoch 67/100, Loss: 1.7901\n",
      "Epoch 68/100, Loss: 1.7516\n",
      "Epoch 69/100, Loss: 1.7138\n",
      "Epoch 70/100, Loss: 1.6767\n",
      "Epoch 71/100, Loss: 1.6403\n",
      "Epoch 72/100, Loss: 1.6045\n",
      "Epoch 73/100, Loss: 1.5695\n",
      "Epoch 74/100, Loss: 1.5351\n",
      "Epoch 75/100, Loss: 1.5014\n",
      "Epoch 76/100, Loss: 1.4684\n",
      "Epoch 77/100, Loss: 1.4361\n",
      "Epoch 78/100, Loss: 1.4044\n",
      "Epoch 79/100, Loss: 1.3734\n",
      "Epoch 80/100, Loss: 1.3431\n",
      "Epoch 81/100, Loss: 1.3134\n",
      "Epoch 82/100, Loss: 1.2843\n",
      "Epoch 83/100, Loss: 1.2559\n",
      "Epoch 84/100, Loss: 1.2281\n",
      "Epoch 85/100, Loss: 1.2010\n",
      "Epoch 86/100, Loss: 1.1745\n",
      "Epoch 87/100, Loss: 1.1485\n",
      "Epoch 88/100, Loss: 1.1232\n",
      "Epoch 89/100, Loss: 1.0985\n",
      "Epoch 90/100, Loss: 1.0743\n",
      "Epoch 91/100, Loss: 1.0507\n",
      "Epoch 92/100, Loss: 1.0277\n",
      "Epoch 93/100, Loss: 1.0053\n",
      "Epoch 94/100, Loss: 0.9833\n",
      "Epoch 95/100, Loss: 0.9620\n",
      "Epoch 96/100, Loss: 0.9411\n",
      "Epoch 97/100, Loss: 0.9208\n",
      "Epoch 98/100, Loss: 0.9009\n",
      "Epoch 99/100, Loss: 0.8816\n",
      "Epoch 100/100, Loss: 0.8627\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (context, target) in enumerate(dataloader):\n",
    "        \n",
    "        context = context.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(context)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def get_similar_words(word: str, \n",
    "                     model: CBOW, \n",
    "                     dataset: CBOWDataset, \n",
    "                     top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Find similar words based on cosine similarity of embeddings\n",
    "    \n",
    "    Args:\n",
    "        word (str): Query word\n",
    "        model (CBOW): Trained CBOW model\n",
    "        dataset (CBOWDataset): Dataset containing vocabulary\n",
    "        top_k (int): Number of similar words to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: List of (word, similarity) pairs\n",
    "    \"\"\"\n",
    "    if word not in dataset.vocab:\n",
    "        raise ValueError(f\"Word '{word}' not in vocabulary\")\n",
    "    \n",
    "    # Get word embedding\n",
    "    word_idx = torch.tensor([dataset.word_to_idx[word]])\n",
    "    word_embedding = model.embeddings(word_idx)\n",
    "    \n",
    "    # Calculate similarities with all words\n",
    "    all_embeddings = model.embeddings.weight.detach()\n",
    "    similarities = torch.cosine_similarity(word_embedding, all_embeddings)\n",
    "    \n",
    "    # Get top-k similar words\n",
    "    top_indices = torch.argsort(similarities, descending=True)[1:top_k+1]\n",
    "    similar_words = [(dataset.idx_to_word[idx.item()], \n",
    "                     similarities[idx].item()) \n",
    "                    for idx in top_indices]\n",
    "    \n",
    "    return similar_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ruins', 0.2600654363632202),\n",
       " ('off', 0.2595617175102234),\n",
       " ('reminder', 0.2342543751001358),\n",
       " ('gust', 0.2105642557144165),\n",
       " ('distant', 0.2053515762090683)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar_words('sun', model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Word 'I' not in vocabulary",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_similar_words\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mI\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[55], line 21\u001b[0m, in \u001b[0;36mget_similar_words\u001b[0;34m(word, model, dataset, top_k)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mFind similar words based on cosine similarity of embeddings\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    List[Tuple[str, float]]: List of (word, similarity) pairs\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mvocab:\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Get word embedding\u001b[39;00m\n\u001b[1;32m     24\u001b[0m word_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([dataset\u001b[38;5;241m.\u001b[39mword_to_idx[word]])\n",
      "\u001b[0;31mValueError\u001b[0m: Word 'I' not in vocabulary"
     ]
    }
   ],
   "source": [
    "get_similar_words('I', model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save & load model \n",
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "def save_model(model: CBOW, dataset: CBOWDataset, save_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Save CBOW model and vocabulary\n",
    "    \n",
    "    Args:\n",
    "        model (CBOW): Trained CBOW model\n",
    "        dataset (CBOWDataset): Dataset containing vocabulary\n",
    "        save_dir (str): Directory to save model and vocabulary\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state\n",
    "    model_path = os.path.join(save_dir, 'cbow_model.pt')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Save vocabulary\n",
    "    vocab_path = os.path.join(save_dir, 'vocabulary.json')\n",
    "    dataset.save_vocab(vocab_path)\n",
    "\n",
    "def load_model(save_dir: str, embedding_dim: int) -> Tuple[CBOW, Dict]:\n",
    "    \"\"\"\n",
    "    Load saved CBOW model and vocabulary\n",
    "    \n",
    "    Args:\n",
    "        save_dir (str): Directory containing saved model and vocabulary\n",
    "        embedding_dim (int): Dimension of word embeddings\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[CBOW, Dict]: Loaded model and vocabulary data\n",
    "    \"\"\"\n",
    "    # Load vocabulary\n",
    "    vocab_path = os.path.join(save_dir, 'vocabulary.json')\n",
    "    word_to_idx, idx_to_word, vocab = CBOWDataset.load_vocab(vocab_path)\n",
    "    \n",
    "    # Initialize and load model\n",
    "    model = CBOW(len(vocab), embedding_dim)\n",
    "    model_path = os.path.join(save_dir, 'cbow_model.pt')\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    return model, word_to_idx\n",
    "\n",
    "def inference(model, word, word_to_idx):\n",
    "    word_idx = torch.tensor([word_to_idx[word]])\n",
    "    word_embedding = model.embeddings(word_idx)\n",
    "    return word_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, dataset, 'cbow_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.5920e-01, -4.0341e-02,  1.5539e+00,  3.2055e-01,  9.5291e-01,\n",
       "          5.9976e-01,  5.4465e-01, -1.6439e+00, -1.5701e+00,  1.1009e-01,\n",
       "          7.8184e-01, -1.0384e+00,  7.7920e-01,  1.2740e+00, -1.0114e+00,\n",
       "          7.6585e-01, -2.7919e-01,  4.0247e-01,  1.0488e+00,  1.9079e+00,\n",
       "         -2.1199e-01, -4.4335e-01, -1.8806e-01,  8.7044e-01, -8.1496e-01,\n",
       "         -1.5402e-01,  2.3751e+00,  1.9763e-01,  2.7805e-01,  3.4879e-01,\n",
       "          7.8925e-01,  1.4039e+00, -1.6240e+00, -1.0708e-01,  2.3802e-01,\n",
       "         -1.2223e+00,  1.1615e+00, -9.0425e-01,  6.5808e-01,  1.8840e-01,\n",
       "          7.5056e-01, -2.1386e-01, -8.5958e-01,  8.0674e-01, -9.8270e-01,\n",
       "          1.4049e-01, -3.6771e-01, -9.4613e-01,  2.1960e-01,  7.4400e-01,\n",
       "          6.8973e-01,  1.6451e+00, -1.3060e+00,  8.5354e-01, -1.1818e+00,\n",
       "         -1.2987e+00, -6.9507e-01,  1.3776e+00,  9.9991e-01, -7.0496e-01,\n",
       "         -1.7938e-03,  2.0736e-01, -2.5317e-01,  4.1819e-01,  5.8782e-02,\n",
       "          2.4086e-01,  2.4027e-01, -4.4859e-01,  2.1126e+00, -3.7047e-01,\n",
       "         -3.8612e-01,  8.0172e-01, -4.9211e-01, -1.0967e+00, -1.8290e-01,\n",
       "         -1.5858e+00, -1.0217e+00,  9.8284e-01, -1.0987e+00,  1.0100e+00,\n",
       "         -3.6834e-01,  5.7893e-01,  1.1769e+00,  9.9184e-01,  1.8433e+00,\n",
       "          1.1973e+00, -1.1511e+00, -3.9046e-01, -2.1364e+00,  4.8815e-01,\n",
       "         -1.1908e+00, -6.8721e-01, -5.6576e-01, -8.7164e-02,  9.3200e-01,\n",
       "          8.7832e-01,  8.5545e-01,  1.4706e-01,  1.7138e-01, -1.0272e+00]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example inference using saved model\n",
    "model, word_to_idx = load_model('cbow_model', embedding_dim)\n",
    "inference(model, 'sun', word_to_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
