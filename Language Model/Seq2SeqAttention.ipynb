{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Attention Mechanism\n",
    "\n",
    "The Attention mechanism is a technique that allows neural networks to focus on specific parts of the input sequence when making predictions. It was introduced to address the limitations of traditional sequence-to-sequence models like LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units), which often struggle with long-range dependencies in sequences.\n",
    "\n",
    "## How Attention Works\n",
    "\n",
    "The core idea of the Attention mechanism is to compute a weighted sum of all input elements, where the weights are dynamically calculated based on the relevance of each input element to the current output element being generated. This allows the model to \"attend\" to different parts of the input sequence as needed.\n",
    "\n",
    "### Steps Involved in Attention Mechanism:\n",
    "\n",
    "1. **Score Calculation**: For each output time step, calculate a score for each input element. This score represents the relevance of the input element to the current output element.\n",
    "2. **Softmax**: Apply the softmax function to the scores to obtain attention weights. These weights sum to 1 and indicate the importance of each input element.\n",
    "3. **Context Vector**: Compute the context vector as the weighted sum of the input elements, using the attention weights.\n",
    "4. **Output Generation**: Use the context vector to generate the output for the current time step.\n",
    "\n",
    "### Types of Attention Mechanisms:\n",
    "\n",
    "- **Global Attention**: Considers all input elements when computing the context vector.\n",
    "- **Local Attention**: Focuses on a subset of input elements, typically around a specific position.\n",
    "\n",
    "## Improvements Over LSTM/GRU\n",
    "\n",
    "1. **Handling Long-Range Dependencies**: Attention mechanisms can effectively capture long-range dependencies in sequences, which LSTMs and GRUs often struggle with.\n",
    "2. **Parallelization**: Attention mechanisms, especially in the Transformer architecture, allow for parallel processing of input sequences, leading to faster training times compared to the sequential nature of LSTMs and GRUs.\n",
    "3. **Interpretability**: The attention weights provide insights into which parts of the input sequence the model is focusing on, making the model more interpretable.\n",
    "\n",
    "Overall, the Attention mechanism has significantly improved the performance of sequence-to-sequence models in various tasks such as machine translation, text summarization, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example implementation of Machine Translate using Bi-LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'The weather is nice today.', 'target': 'Thời tiết hôm nay thật đẹp.'}\n",
      "{'source': 'I love programming.', 'target': 'Tôi yêu lập trình.'}\n",
      "{'source': 'How old are you?', 'target': 'Bạn bao nhiêu tuổi?'}\n",
      "{'source': 'Where do you live?', 'target': 'Bạn sống ở đâu?'}\n",
      "{'source': 'What is your favorite color?', 'target': 'Màu sắc yêu thích của bạn là gì?'}\n"
     ]
    }
   ],
   "source": [
    "source_sentences = [\n",
    "    \"The weather is nice today.\",\n",
    "    \"I love programming.\",\n",
    "    \"How old are you?\",\n",
    "    \"Where do you live?\",\n",
    "    \"What is your favorite color?\",\n",
    "    \"I enjoy reading books.\",\n",
    "    \"Do you like music?\",\n",
    "    \"What time is it?\",\n",
    "    \"Can you help me?\",\n",
    "    \"I am going to the store.\",\n",
    "    \"She is my best friend.\",\n",
    "    \"We are having dinner.\",\n",
    "    \"This is a beautiful place.\",\n",
    "    \"I need to study for my exam.\",\n",
    "    \"He is a great teacher.\",\n",
    "    \"They are playing soccer.\",\n",
    "    \"I want to learn Spanish.\",\n",
    "    \"Do you speak English?\",\n",
    "    \"I have a pet dog.\",\n",
    "    \"She likes to dance.\",\n",
    "    \"We are watching a movie.\",\n",
    "    \"This is my favorite song.\",\n",
    "    \"I am feeling happy today.\",\n",
    "    \"He is very talented.\",\n",
    "    \"They are traveling to Europe.\",\n",
    "    \"I need to buy groceries.\",\n",
    "    \"She is reading a novel.\",\n",
    "    \"We are going to the beach.\",\n",
    "    \"This is an interesting book.\",\n",
    "    \"I am learning to cook.\",\n",
    "    \"He is working on a project.\",\n",
    "    \"They are visiting their grandparents.\",\n",
    "    \"I want to go for a walk.\",\n",
    "    \"Do you like to swim?\",\n",
    "    \"I have a lot of homework.\",\n",
    "    \"She is painting a picture.\",\n",
    "    \"We are celebrating a birthday.\",\n",
    "    \"This is a challenging task.\",\n",
    "    \"I am practicing yoga.\"\n",
    "]\n",
    "\n",
    "target_sentences = [\n",
    "    \"Thời tiết hôm nay thật đẹp.\",\n",
    "    \"Tôi yêu lập trình.\",\n",
    "    \"Bạn bao nhiêu tuổi?\",\n",
    "    \"Bạn sống ở đâu?\",\n",
    "    \"Màu sắc yêu thích của bạn là gì?\",\n",
    "    \"Tôi thích đọc sách.\",\n",
    "    \"Bạn có thích âm nhạc không?\",\n",
    "    \"Mấy giờ rồi?\",\n",
    "    \"Bạn có thể giúp tôi không?\",\n",
    "    \"Tôi đang đi đến cửa hàng.\",\n",
    "    \"Cô ấy là bạn thân nhất của tôi.\",\n",
    "    \"Chúng tôi đang ăn tối.\",\n",
    "    \"Đây là một nơi đẹp.\",\n",
    "    \"Tôi cần học cho kỳ thi của mình.\",\n",
    "    \"Anh ấy là một giáo viên tuyệt vời.\",\n",
    "    \"Họ đang chơi bóng đá.\",\n",
    "    \"Tôi muốn học tiếng Tây Ban Nha.\",\n",
    "    \"Bạn có nói tiếng Anh không?\",\n",
    "    \"Tôi có một con chó cưng.\",\n",
    "    \"Cô ấy thích nhảy múa.\",\n",
    "    \"Chúng tôi đang xem phim.\",\n",
    "    \"Đây là bài hát yêu thích của tôi.\",\n",
    "    \"Hôm nay tôi cảm thấy hạnh phúc.\",\n",
    "    \"Anh ấy rất tài năng.\",\n",
    "    \"Họ đang du lịch đến Châu Âu.\",\n",
    "    \"Tôi cần mua hàng tạp hóa.\",\n",
    "    \"Cô ấy đang đọc một cuốn tiểu thuyết.\",\n",
    "    \"Chúng tôi đang đi đến bãi biển.\",\n",
    "    \"Đây là một cuốn sách thú vị.\",\n",
    "    \"Tôi đang học nấu ăn.\",\n",
    "    \"Anh ấy đang làm việc trên một dự án.\",\n",
    "    \"Họ đang thăm ông bà của họ.\",\n",
    "    \"Tôi muốn đi dạo.\",\n",
    "    \"Bạn có thích bơi không?\",\n",
    "    \"Tôi có rất nhiều bài tập về nhà.\",\n",
    "    \"Cô ấy đang vẽ một bức tranh.\",\n",
    "    \"Chúng tôi đang tổ chức sinh nhật.\",\n",
    "    \"Đây là một nhiệm vụ khó khăn.\",\n",
    "    \"Tôi đang tập yoga.\"\n",
    "]\n",
    "\n",
    "# Create random translation data\n",
    "translation_data = [{\"source\": src, \"target\": tgt} for src, tgt in zip(source_sentences, target_sentences)]\n",
    "\n",
    "# Save into a list\n",
    "train_data = list(translation_data)\n",
    "\n",
    "# Print example data\n",
    "for example in train_data[:5]:  # Show first 5 examples\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data \n",
    "word2index = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "\n",
    "# add source_sentences & target_sentences to word2index\n",
    "for example in train_data:\n",
    "    for word in example[\"source\"].split():\n",
    "        if word not in word2index:\n",
    "            word2index[word] = len(word2index)\n",
    "    for word in example[\"target\"].split():\n",
    "        if word not in word2index:\n",
    "            word2index[word] = len(word2index)\n",
    "\n",
    "index2word = {index: word for word, index in word2index.items()}\n",
    "vocab_size = len(word2index)\n",
    "\n",
    "class MachineTranslateDataset:\n",
    "\n",
    "    def __init__(self, data, word2index, max_length=50):\n",
    "        self.data = data\n",
    "        self.word2index = word2index\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def pad_sequence(self, sequence, max_len):\n",
    "        # Truncate if sequence is longer than max_len\n",
    "        if len(sequence) > max_len:\n",
    "            return sequence[:max_len]\n",
    "        # Pad with <pad> token if sequence is shorter\n",
    "        else:\n",
    "            return sequence + [self.word2index[\"<pad>\"]] * (max_len - len(sequence))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        src = self.data[index][\"source\"]\n",
    "        trg = self.data[index][\"target\"]\n",
    "        \n",
    "        # Convert words to indices\n",
    "        src_indexes = [self.word2index.get(word, self.word2index[\"<unk>\"]) \n",
    "                      for word in src.split()]\n",
    "        trg_indexes = [self.word2index.get(word, self.word2index[\"<unk>\"]) \n",
    "                      for word in trg.split()]\n",
    "        \n",
    "        # Add <sos> and <eos> tokens\n",
    "        src_indexes = [self.word2index[\"<sos>\"]] + src_indexes + [self.word2index[\"<eos>\"]]\n",
    "        trg_indexes = [self.word2index[\"<sos>\"]] + trg_indexes + [self.word2index[\"<eos>\"]]\n",
    "        \n",
    "        # Pad sequences\n",
    "        src_indexes = self.pad_sequence(src_indexes, self.max_length)\n",
    "        trg_indexes = self.pad_sequence(trg_indexes, self.max_length)\n",
    "        \n",
    "        return (torch.tensor(src_indexes, dtype=torch.long), \n",
    "                torch.tensor(trg_indexes, dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 4, 5, 6, 7, 8, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]),\n",
       " tensor([ 1,  9, 10, 11, 12, 13, 14,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_translate_dataset =  MachineTranslateDataset(translation_data, word2index)\n",
    "machine_translate_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 512]) torch.Size([2, 1, 512]) torch.Size([2, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "    \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        # embedded = [ batch size, seq_length, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs = [batch size, seq_length, hidden dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        \n",
    "        # n directions =  1 if LSTM, n directions = 2 if bidirectional LSTM\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "encoder = Encoder(vocab_size, 256, 512, 2, 0.5)\n",
    "outputs, hidden, cell = encoder(torch.tensor([[1, 2, 3, 4, 5]]))\n",
    "print(outputs.shape, hidden.shape, cell.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luong Attention Mechanism - Quick Overview\n",
    "\n",
    "## Core Concept\n",
    "Luong attention calculates attention weights between encoder and decoder hidden states to help the decoder focus on relevant input parts during sequence generation.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### 1. Three Score Functions\n",
    "$$\n",
    "\\text{Dot:} \\quad score(h_t, \\bar{h}_s) = h_t^T\\bar{h}_s\n",
    "$$\n",
    "$$\n",
    "\\text{General:} \\quad score(h_t, \\bar{h}_s) = h_t^TW_a\\bar{h}_s\n",
    "$$\n",
    "$$\n",
    "\\text{Concat:} \\quad score(h_t, \\bar{h}_s) = v_a^T\\tanh(W_a[h_t;\\bar{h}_s])\n",
    "$$\n",
    "\n",
    "### 2. Attention Weight Calculation\n",
    "$$\n",
    "\\alpha_{ts} = \\text{softmax}(score(h_t, \\bar{h}_s))\n",
    "$$\n",
    "\n",
    "### 3. Context Vector\n",
    "$$\n",
    "c_t = \\sum_s \\alpha_{ts}\\bar{h}_s\n",
    "$$\n",
    "\n",
    "## Key Features\n",
    "1. Offers both global and local attention variants\n",
    "2. Uses input feeding for richer representations\n",
    "3. Simpler than Bahdanau attention\n",
    "4. Computationally efficient\n",
    "\n",
    "## Common Use Cases\n",
    "- Neural Machine Translation\n",
    "- Text Summarization\n",
    "- Question Answering\n",
    "- Speech Recognition\n",
    "\n",
    "## Implementation Tips\n",
    "1. Start with dot product scoring\n",
    "2. Use global attention for short sequences\n",
    "3. Consider local attention for long sequences\n",
    "4. Monitor attention weights during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "class LuongAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # encoder_outputs = [batch size, seq length, hidden dim]\n",
    "        \n",
    "\n",
    "        print(hidden.shape, encoder_outputs.shape)\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        seq_length = encoder_outputs.shape[1]\n",
    "        \n",
    "        # Repeat hidden state for each sequence in the input\n",
    "        hidden = hidden[-1].unsqueeze(1).repeat(1, seq_length, 1)\n",
    "        # hidden = [batch size, seq length, hidden dim]\n",
    "\n",
    "        # Calculate attention scores\n",
    "        attention_scores = torch.bmm(hidden, encoder_outputs.permute(0, 2, 1)).squeeze(1)\n",
    "        # attention_scores = [batch size, seq length]\n",
    "        \n",
    "        return F.softmax(attention_scores, dim=1)\n",
    "        # attention_weights = [batch size, seq length]\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_dim, hidden_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, hidden_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(hidden_dim * 2, hidden_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # input = [batch size]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # encoder_outputs = [batch size, seq length, hidden dim]\n",
    "        \n",
    "        print(\"Input shape for decoder: \", hidden.shape, encoder_outputs.shape)\n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch size, hidden dim]\n",
    "        \n",
    "        attention_weights = self.attention(hidden, encoder_outputs)\n",
    "        # attention_weights = [batch size, seq length]\n",
    "        \n",
    "        attention_weights = attention_weights.unsqueeze(1)\n",
    "        # attention_weights = [batch size, 1, seq length]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # encoder_outputs = [seq length, batch size, hidden dim]\n",
    "        \n",
    "        weighted = torch.bmm(attention_weights, encoder_outputs)\n",
    "        # weighted = [batch size, 1, hidden dim]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        # weighted = [1, batch size, hidden dim]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        # rnn_input = [1, batch size, hidden dim * 2]\n",
    "        \n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        # output = [seq length, batch size, hidden dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        # prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden\n",
    "\n",
    "class Seq2SeqAtt(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src = [batch size, src len]\n",
    "        # trg = [batch size, src len]\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        # hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        print(\"Encoder output shape: \", encoder_outputs.shape, hidden.shape, cell.shape)\n",
    "\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "\n",
    "            outputs[:, t] = output\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            top1 = output.argmax(1) \n",
    "\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqAtt(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(243, 100)\n",
      "    (rnn): LSTM(100, 256, num_layers=3, batch_first=True, dropout=0.5)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): LuongAttention()\n",
      "    (embedding): Embedding(243, 256)\n",
      "    (rnn): GRU(512, 256, num_layers=3, dropout=0.5)\n",
      "    (fc_out): Linear(in_features=256, out_features=243, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "Encoder output shape:  torch.Size([8, 50, 256]) torch.Size([3, 8, 256]) torch.Size([3, 8, 256])\n",
      "Input shape for decoder:  torch.Size([3, 8, 256]) torch.Size([8, 50, 256])\n",
      "torch.Size([3, 8, 256]) torch.Size([8, 50, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "batch1 must be a 3D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (source, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m     37\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 38\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     output_dim \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     40\u001b[0m     output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output_dim)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[52], line 117\u001b[0m, in \u001b[0;36mSeq2SeqAtt.forward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m trg[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, trg_len):\n\u001b[0;32m--> 117\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     outputs[:, t] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    121\u001b[0m     teacher_force \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m teacher_forcing_ratio\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[52], line 68\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     65\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m encoder_outputs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# encoder_outputs = [seq length, batch size, hidden dim]\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m weighted \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# weighted = [batch size, 1, hidden dim]\u001b[39;00m\n\u001b[1;32m     71\u001b[0m weighted \u001b[38;5;241m=\u001b[39m weighted\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: batch1 must be a 3D tensor"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "VOCAB_SIZE = len(word2index)\n",
    "EMB_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_DROP = 0.5\n",
    "DEC_DROP = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "enc = Encoder(VOCAB_SIZE, EMB_DIM, HIDDEN_DIM, ENC_LAYERS, ENC_DROP)\n",
    "attn = LuongAttention(HIDDEN_DIM)\n",
    "dec = Decoder(VOCAB_SIZE, HIDDEN_DIM, DEC_LAYERS, DEC_DROP, attn)\n",
    "model = Seq2SeqAtt(enc, dec, device).to(device)\n",
    "print(model)\n",
    "\n",
    "# Create Dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "data_loader = DataLoader(machine_translate_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# define hypermeters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, (source, target) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(source, target)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        target = target[1:].view(-1)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += loss.item()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}\")\n",
    "    \n",
    "    # Print epoch statistics\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
