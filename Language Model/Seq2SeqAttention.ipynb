{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Attention Mechanism\n",
    "\n",
    "The Attention mechanism is a technique that allows neural networks to focus on specific parts of the input sequence when making predictions. It was introduced to address the limitations of traditional sequence-to-sequence models like LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units), which often struggle with long-range dependencies in sequences.\n",
    "\n",
    "## How Attention Works\n",
    "\n",
    "The core idea of the Attention mechanism is to compute a weighted sum of all input elements, where the weights are dynamically calculated based on the relevance of each input element to the current output element being generated. This allows the model to \"attend\" to different parts of the input sequence as needed.\n",
    "\n",
    "### Steps Involved in Attention Mechanism:\n",
    "\n",
    "1. **Score Calculation**: For each output time step, calculate a score for each input element. This score represents the relevance of the input element to the current output element.\n",
    "2. **Softmax**: Apply the softmax function to the scores to obtain attention weights. These weights sum to 1 and indicate the importance of each input element.\n",
    "3. **Context Vector**: Compute the context vector as the weighted sum of the input elements, using the attention weights.\n",
    "4. **Output Generation**: Use the context vector to generate the output for the current time step.\n",
    "\n",
    "### Types of Attention Mechanisms:\n",
    "\n",
    "- **Global Attention**: Considers all input elements when computing the context vector.\n",
    "- **Local Attention**: Focuses on a subset of input elements, typically around a specific position.\n",
    "\n",
    "## Improvements Over LSTM/GRU\n",
    "\n",
    "1. **Handling Long-Range Dependencies**: Attention mechanisms can effectively capture long-range dependencies in sequences, which LSTMs and GRUs often struggle with.\n",
    "2. **Parallelization**: Attention mechanisms, especially in the Transformer architecture, allow for parallel processing of input sequences, leading to faster training times compared to the sequential nature of LSTMs and GRUs.\n",
    "3. **Interpretability**: The attention weights provide insights into which parts of the input sequence the model is focusing on, making the model more interpretable.\n",
    "\n",
    "Overall, the Attention mechanism has significantly improved the performance of sequence-to-sequence models in various tasks such as machine translation, text summarization, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example implementation of Machine Translate using Bi-LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'The weather is nice today.', 'target': 'Thời tiết hôm nay thật đẹp.'}\n",
      "{'source': 'I love programming.', 'target': 'Tôi yêu lập trình.'}\n",
      "{'source': 'How old are you?', 'target': 'Bạn bao nhiêu tuổi?'}\n",
      "{'source': 'Where do you live?', 'target': 'Bạn sống ở đâu?'}\n",
      "{'source': 'What is your favorite color?', 'target': 'Màu sắc yêu thích của bạn là gì?'}\n"
     ]
    }
   ],
   "source": [
    "source_sentences = [\n",
    "    \"The weather is nice today.\",\n",
    "    \"I love programming.\",\n",
    "    \"How old are you?\",\n",
    "    \"Where do you live?\",\n",
    "    \"What is your favorite color?\",\n",
    "    \"I enjoy reading books.\",\n",
    "    \"Do you like music?\",\n",
    "    \"What time is it?\",\n",
    "    \"Can you help me?\",\n",
    "    \"I am going to the store.\",\n",
    "    \"She is my best friend.\",\n",
    "    \"We are having dinner.\",\n",
    "    \"This is a beautiful place.\",\n",
    "    \"I need to study for my exam.\",\n",
    "    \"He is a great teacher.\",\n",
    "    \"They are playing soccer.\",\n",
    "    \"I want to learn Spanish.\",\n",
    "    \"Do you speak English?\",\n",
    "    \"I have a pet dog.\",\n",
    "    \"She likes to dance.\",\n",
    "    \"We are watching a movie.\",\n",
    "    \"This is my favorite song.\",\n",
    "    \"I am feeling happy today.\",\n",
    "    \"He is very talented.\",\n",
    "    \"They are traveling to Europe.\",\n",
    "    \"I need to buy groceries.\",\n",
    "    \"She is reading a novel.\",\n",
    "    \"We are going to the beach.\",\n",
    "    \"This is an interesting book.\",\n",
    "    \"I am learning to cook.\",\n",
    "    \"He is working on a project.\",\n",
    "    \"They are visiting their grandparents.\",\n",
    "    \"I want to go for a walk.\",\n",
    "    \"Do you like to swim?\",\n",
    "    \"I have a lot of homework.\",\n",
    "    \"She is painting a picture.\",\n",
    "    \"We are celebrating a birthday.\",\n",
    "    \"This is a challenging task.\",\n",
    "    \"I am practicing yoga.\"\n",
    "]\n",
    "\n",
    "target_sentences = [\n",
    "    \"Thời tiết hôm nay thật đẹp.\",\n",
    "    \"Tôi yêu lập trình.\",\n",
    "    \"Bạn bao nhiêu tuổi?\",\n",
    "    \"Bạn sống ở đâu?\",\n",
    "    \"Màu sắc yêu thích của bạn là gì?\",\n",
    "    \"Tôi thích đọc sách.\",\n",
    "    \"Bạn có thích âm nhạc không?\",\n",
    "    \"Mấy giờ rồi?\",\n",
    "    \"Bạn có thể giúp tôi không?\",\n",
    "    \"Tôi đang đi đến cửa hàng.\",\n",
    "    \"Cô ấy là bạn thân nhất của tôi.\",\n",
    "    \"Chúng tôi đang ăn tối.\",\n",
    "    \"Đây là một nơi đẹp.\",\n",
    "    \"Tôi cần học cho kỳ thi của mình.\",\n",
    "    \"Anh ấy là một giáo viên tuyệt vời.\",\n",
    "    \"Họ đang chơi bóng đá.\",\n",
    "    \"Tôi muốn học tiếng Tây Ban Nha.\",\n",
    "    \"Bạn có nói tiếng Anh không?\",\n",
    "    \"Tôi có một con chó cưng.\",\n",
    "    \"Cô ấy thích nhảy múa.\",\n",
    "    \"Chúng tôi đang xem phim.\",\n",
    "    \"Đây là bài hát yêu thích của tôi.\",\n",
    "    \"Hôm nay tôi cảm thấy hạnh phúc.\",\n",
    "    \"Anh ấy rất tài năng.\",\n",
    "    \"Họ đang du lịch đến Châu Âu.\",\n",
    "    \"Tôi cần mua hàng tạp hóa.\",\n",
    "    \"Cô ấy đang đọc một cuốn tiểu thuyết.\",\n",
    "    \"Chúng tôi đang đi đến bãi biển.\",\n",
    "    \"Đây là một cuốn sách thú vị.\",\n",
    "    \"Tôi đang học nấu ăn.\",\n",
    "    \"Anh ấy đang làm việc trên một dự án.\",\n",
    "    \"Họ đang thăm ông bà của họ.\",\n",
    "    \"Tôi muốn đi dạo.\",\n",
    "    \"Bạn có thích bơi không?\",\n",
    "    \"Tôi có rất nhiều bài tập về nhà.\",\n",
    "    \"Cô ấy đang vẽ một bức tranh.\",\n",
    "    \"Chúng tôi đang tổ chức sinh nhật.\",\n",
    "    \"Đây là một nhiệm vụ khó khăn.\",\n",
    "    \"Tôi đang tập yoga.\"\n",
    "]\n",
    "\n",
    "# Create random translation data\n",
    "translation_data = [{\"source\": src, \"target\": tgt} for src, tgt in zip(source_sentences, target_sentences)]\n",
    "\n",
    "# Save into a list\n",
    "train_data = list(translation_data)\n",
    "\n",
    "# Print example data\n",
    "for example in train_data[:5]:  # Show first 5 examples\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data \n",
    "word2index = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "\n",
    "# add source_sentences & target_sentences to word2index\n",
    "for example in train_data:\n",
    "    for word in example[\"source\"].split():\n",
    "        if word not in word2index:\n",
    "            word2index[word] = len(word2index)\n",
    "    for word in example[\"target\"].split():\n",
    "        if word not in word2index:\n",
    "            word2index[word] = len(word2index)\n",
    "\n",
    "index2word = {index: word for word, index in word2index.items()}\n",
    "vocab_size = len(word2index)\n",
    "\n",
    "class MachineTranslateDataset:\n",
    "\n",
    "    def __init__(self, data, word2index, max_length=50):\n",
    "        self.data = data\n",
    "        self.word2index = word2index\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def pad_sequence(self, sequence, max_len):\n",
    "        # Truncate if sequence is longer than max_len\n",
    "        if len(sequence) > max_len:\n",
    "            return sequence[:max_len]\n",
    "        # Pad with <pad> token if sequence is shorter\n",
    "        else:\n",
    "            return sequence + [self.word2index[\"<pad>\"]] * (max_len - len(sequence))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        src = self.data[index][\"source\"]\n",
    "        trg = self.data[index][\"target\"]\n",
    "        \n",
    "        # Convert words to indices\n",
    "        src_indexes = [self.word2index.get(word, self.word2index[\"<unk>\"]) \n",
    "                      for word in src.split()]\n",
    "        trg_indexes = [self.word2index.get(word, self.word2index[\"<unk>\"]) \n",
    "                      for word in trg.split()]\n",
    "        \n",
    "        # Add <sos> and <eos> tokens\n",
    "        src_indexes = [self.word2index[\"<sos>\"]] + src_indexes + [self.word2index[\"<eos>\"]]\n",
    "        trg_indexes = [self.word2index[\"<sos>\"]] + trg_indexes + [self.word2index[\"<eos>\"]]\n",
    "        \n",
    "        # Pad sequences\n",
    "        src_indexes = self.pad_sequence(src_indexes, self.max_length)\n",
    "        trg_indexes = self.pad_sequence(trg_indexes, self.max_length)\n",
    "        \n",
    "        return (torch.tensor(src_indexes, dtype=torch.long), \n",
    "                torch.tensor(trg_indexes, dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 4, 5, 6, 7, 8, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]),\n",
       " tensor([ 1,  9, 10, 11, 12, 13, 14,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_translate_dataset =  MachineTranslateDataset(translation_data, word2index)\n",
    "machine_translate_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 512]) torch.Size([2, 1, 512]) torch.Size([2, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "    \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        # embedded = [ batch size, seq_length, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs = [batch size, seq_length, hidden dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        \n",
    "        # n directions =  1 if LSTM, n directions = 2 if bidirectional LSTM\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "encoder = Encoder(vocab_size, 256, 512, 2, 0.5)\n",
    "outputs, hidden, cell = encoder(torch.tensor([[1, 2, 3, 4, 5]]))\n",
    "print(outputs.shape, hidden.shape, cell.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luong Attention Mechanism - Quick Overview\n",
    "\n",
    "## Core Concept\n",
    "Luong attention calculates attention weights between encoder and decoder hidden states to help the decoder focus on relevant input parts during sequence generation.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### 1. Three Score Functions\n",
    "$$\n",
    "\\text{Dot:} \\quad score(h_t, \\bar{h}_s) = h_t^T\\bar{h}_s\n",
    "$$\n",
    "$$\n",
    "\\text{General:} \\quad score(h_t, \\bar{h}_s) = h_t^TW_a\\bar{h}_s\n",
    "$$\n",
    "$$\n",
    "\\text{Concat:} \\quad score(h_t, \\bar{h}_s) = v_a^T\\tanh(W_a[h_t;\\bar{h}_s])\n",
    "$$\n",
    "\n",
    "### 2. Attention Weight Calculation\n",
    "$$\n",
    "\\alpha_{ts} = \\text{softmax}(score(h_t, \\bar{h}_s))\n",
    "$$\n",
    "\n",
    "### 3. Context Vector\n",
    "$$\n",
    "c_t = \\sum_s \\alpha_{ts}\\bar{h}_s\n",
    "$$\n",
    "\n",
    "## Key Features\n",
    "1. Offers both global and local attention variants\n",
    "2. Uses input feeding for richer representations\n",
    "3. Simpler than Bahdanau attention\n",
    "4. Computationally efficient\n",
    "\n",
    "## Common Use Cases\n",
    "- Neural Machine Translation\n",
    "- Text Summarization\n",
    "- Question Answering\n",
    "- Speech Recognition\n",
    "\n",
    "## Implementation Tips\n",
    "1. Start with dot product scoring\n",
    "2. Use global attention for short sequences\n",
    "3. Consider local attention for long sequences\n",
    "4. Monitor attention weights during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "\n",
    "        \"\"\"\n",
    "        Compute dot attention weights and weighted sum of encoder outputs\n",
    "\n",
    "        Args:\n",
    "            decoder_hidden: Current decoder hidden state\n",
    "                Shape: [n layers * n directions, batch size, hidden dim]\n",
    "            encoder_outputs: All encoder hidden states\n",
    "                Shape: [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        Returns:\n",
    "            attention_weights: Attention weights for each encoder state\n",
    "                Shape: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "\n",
    "        latest_decode_hidden = decoder_hidden[-1].unsqueeze(1)\n",
    "        # latest_decode_hidden = [batch_size, 1, hidden_dim] x [batch_size, hidden_dim, seq_len] = [batch_size, 1, seq_len]\n",
    "\n",
    "        scores = torch.bmm(latest_decode_hidden, encoder_outputs.transpose(1, 2)).squeeze(1)\n",
    "\n",
    "        attention_weights = F.softmax(scores, dim=1)\n",
    "\n",
    "        return attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10])\n"
     ]
    }
   ],
   "source": [
    "# Example test dot LuongAttention\n",
    "attention = LuongAttention(512)\n",
    "example_hidden = torch.randn(3, 8, 256)\n",
    "example_encoder_outputs = torch.randn(8, 10, 256)\n",
    "attention_weights = attention(example_hidden, example_encoder_outputs)\n",
    "print(attention_weights.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_dim, hidden_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, hidden_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(hidden_dim * 2, hidden_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # input = [batch size]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # encoder_outputs = [batch size, seq length, hidden dim]\n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch size, hidden dim]\n",
    "        \n",
    "        attention_weights = self.attention(hidden, encoder_outputs)\n",
    "        # attention_weights = [batch size, seq length]\n",
    "        \n",
    "        attention_weights = attention_weights.unsqueeze(1)\n",
    "        # attention_weights = [batch size, 1, seq length]\n",
    "        \n",
    "        weighted = torch.bmm(attention_weights, encoder_outputs)\n",
    "        # weighted = [batch size, 1, hidden dim]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        # weighted = [1, batch size, hidden dim]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        # rnn_input = [1, batch size, hidden dim * 2]\n",
    "        \n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        # output = [seq length, batch size, hidden dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        # prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden\n",
    "\n",
    "class Seq2SeqAtt(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src = [batch size, src len]\n",
    "        # trg = [batch size, src len]\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        # hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "\n",
    "            outputs[:, t] = output\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            top1 = output.argmax(1) \n",
    "\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqAtt(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(243, 100)\n",
      "    (rnn): LSTM(100, 256, num_layers=3, batch_first=True, dropout=0.5)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): LuongAttention()\n",
      "    (embedding): Embedding(243, 256)\n",
      "    (rnn): GRU(512, 256, num_layers=3, dropout=0.5)\n",
      "    (fc_out): Linear(in_features=256, out_features=243, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "Epoch [1/200], Average Loss: 3.4211\n",
      "Epoch [2/200], Average Loss: 1.0490\n",
      "Epoch [3/200], Average Loss: 0.9375\n",
      "Epoch [4/200], Average Loss: 0.8560\n",
      "Epoch [5/200], Average Loss: 0.8272\n",
      "Epoch [6/200], Average Loss: 0.7661\n",
      "Epoch [7/200], Average Loss: 0.7305\n",
      "Epoch [8/200], Average Loss: 0.7366\n",
      "Epoch [9/200], Average Loss: 0.6932\n",
      "Epoch [10/200], Average Loss: 0.6986\n",
      "Epoch [11/200], Average Loss: 0.6657\n",
      "Epoch [12/200], Average Loss: 0.6929\n",
      "Epoch [13/200], Average Loss: 0.6838\n",
      "Epoch [14/200], Average Loss: 0.6788\n",
      "Epoch [15/200], Average Loss: 0.6586\n",
      "Epoch [16/200], Average Loss: 0.6456\n",
      "Epoch [17/200], Average Loss: 0.6558\n",
      "Epoch [18/200], Average Loss: 0.6630\n",
      "Epoch [19/200], Average Loss: 0.6473\n",
      "Epoch [20/200], Average Loss: 0.6448\n",
      "Epoch [21/200], Average Loss: 0.6349\n",
      "Epoch [22/200], Average Loss: 0.6290\n",
      "Epoch [23/200], Average Loss: 0.6159\n",
      "Epoch [24/200], Average Loss: 0.6148\n",
      "Epoch [25/200], Average Loss: 0.6271\n",
      "Epoch [26/200], Average Loss: 0.6308\n",
      "Epoch [27/200], Average Loss: 0.5914\n",
      "Epoch [28/200], Average Loss: 0.6042\n",
      "Epoch [29/200], Average Loss: 0.6187\n",
      "Epoch [30/200], Average Loss: 0.5838\n",
      "Epoch [31/200], Average Loss: 0.6097\n",
      "Epoch [32/200], Average Loss: 0.5978\n",
      "Epoch [33/200], Average Loss: 0.5890\n",
      "Epoch [34/200], Average Loss: 0.5928\n",
      "Epoch [35/200], Average Loss: 0.6018\n",
      "Epoch [36/200], Average Loss: 0.6038\n",
      "Epoch [37/200], Average Loss: 0.5983\n",
      "Epoch [38/200], Average Loss: 0.6030\n",
      "Epoch [39/200], Average Loss: 0.5896\n",
      "Epoch [40/200], Average Loss: 0.5917\n",
      "Epoch [41/200], Average Loss: 0.5759\n",
      "Epoch [42/200], Average Loss: 0.5897\n",
      "Epoch [43/200], Average Loss: 0.5817\n",
      "Epoch [44/200], Average Loss: 0.5756\n",
      "Epoch [45/200], Average Loss: 0.5732\n",
      "Epoch [46/200], Average Loss: 0.5695\n",
      "Epoch [47/200], Average Loss: 0.5668\n",
      "Epoch [48/200], Average Loss: 0.5631\n",
      "Epoch [49/200], Average Loss: 0.5818\n",
      "Epoch [50/200], Average Loss: 0.5534\n",
      "Epoch [51/200], Average Loss: 0.5714\n",
      "Epoch [52/200], Average Loss: 0.5444\n",
      "Epoch [53/200], Average Loss: 0.5734\n",
      "Epoch [54/200], Average Loss: 0.5829\n",
      "Epoch [55/200], Average Loss: 0.5633\n",
      "Epoch [56/200], Average Loss: 0.5439\n",
      "Epoch [57/200], Average Loss: 0.5656\n",
      "Epoch [58/200], Average Loss: 0.5515\n",
      "Epoch [59/200], Average Loss: 0.5403\n",
      "Epoch [60/200], Average Loss: 0.5492\n",
      "Epoch [61/200], Average Loss: 0.5364\n",
      "Epoch [62/200], Average Loss: 0.5446\n",
      "Epoch [63/200], Average Loss: 0.5361\n",
      "Epoch [64/200], Average Loss: 0.5253\n",
      "Epoch [65/200], Average Loss: 0.5455\n",
      "Epoch [66/200], Average Loss: 0.5116\n",
      "Epoch [67/200], Average Loss: 0.5423\n",
      "Epoch [68/200], Average Loss: 0.5249\n",
      "Epoch [69/200], Average Loss: 0.5413\n",
      "Epoch [70/200], Average Loss: 0.5368\n",
      "Epoch [71/200], Average Loss: 0.5310\n",
      "Epoch [72/200], Average Loss: 0.5376\n",
      "Epoch [73/200], Average Loss: 0.5404\n",
      "Epoch [74/200], Average Loss: 0.5285\n",
      "Epoch [75/200], Average Loss: 0.5192\n",
      "Epoch [76/200], Average Loss: 0.5250\n",
      "Epoch [77/200], Average Loss: 0.5235\n",
      "Epoch [78/200], Average Loss: 0.5065\n",
      "Epoch [79/200], Average Loss: 0.5447\n",
      "Epoch [80/200], Average Loss: 0.4893\n",
      "Epoch [81/200], Average Loss: 0.5227\n",
      "Epoch [82/200], Average Loss: 0.5277\n",
      "Epoch [83/200], Average Loss: 0.4796\n",
      "Epoch [84/200], Average Loss: 0.5337\n",
      "Epoch [85/200], Average Loss: 0.5340\n",
      "Epoch [86/200], Average Loss: 0.5290\n",
      "Epoch [87/200], Average Loss: 0.5206\n",
      "Epoch [88/200], Average Loss: 0.4937\n",
      "Epoch [89/200], Average Loss: 0.5240\n",
      "Epoch [90/200], Average Loss: 0.4661\n",
      "Epoch [91/200], Average Loss: 0.4868\n",
      "Epoch [92/200], Average Loss: 0.4753\n",
      "Epoch [93/200], Average Loss: 0.4936\n",
      "Epoch [94/200], Average Loss: 0.5035\n",
      "Epoch [95/200], Average Loss: 0.5074\n",
      "Epoch [96/200], Average Loss: 0.5034\n",
      "Epoch [97/200], Average Loss: 0.4580\n",
      "Epoch [98/200], Average Loss: 0.4724\n",
      "Epoch [99/200], Average Loss: 0.4727\n",
      "Epoch [100/200], Average Loss: 0.4859\n",
      "Epoch [101/200], Average Loss: 0.4841\n",
      "Epoch [102/200], Average Loss: 0.5063\n",
      "Epoch [103/200], Average Loss: 0.4772\n",
      "Epoch [104/200], Average Loss: 0.4509\n",
      "Epoch [105/200], Average Loss: 0.4922\n",
      "Epoch [106/200], Average Loss: 0.5504\n",
      "Epoch [107/200], Average Loss: 0.5255\n",
      "Epoch [108/200], Average Loss: 0.4983\n",
      "Epoch [109/200], Average Loss: 0.5190\n",
      "Epoch [110/200], Average Loss: 0.4801\n",
      "Epoch [111/200], Average Loss: 0.4920\n",
      "Epoch [112/200], Average Loss: 0.5062\n",
      "Epoch [113/200], Average Loss: 0.4839\n",
      "Epoch [114/200], Average Loss: 0.4448\n",
      "Epoch [115/200], Average Loss: 0.4661\n",
      "Epoch [116/200], Average Loss: 0.4544\n",
      "Epoch [117/200], Average Loss: 0.4330\n",
      "Epoch [118/200], Average Loss: 0.4306\n",
      "Epoch [119/200], Average Loss: 0.4678\n",
      "Epoch [120/200], Average Loss: 0.4806\n",
      "Epoch [121/200], Average Loss: 0.5131\n",
      "Epoch [122/200], Average Loss: 0.4480\n",
      "Epoch [123/200], Average Loss: 0.4269\n",
      "Epoch [124/200], Average Loss: 0.5209\n",
      "Epoch [125/200], Average Loss: 0.4184\n",
      "Epoch [126/200], Average Loss: 0.4067\n",
      "Epoch [127/200], Average Loss: 0.4172\n",
      "Epoch [128/200], Average Loss: 0.4440\n",
      "Epoch [129/200], Average Loss: 0.4320\n",
      "Epoch [130/200], Average Loss: 0.4562\n",
      "Epoch [131/200], Average Loss: 0.4723\n",
      "Epoch [132/200], Average Loss: 0.4220\n",
      "Epoch [133/200], Average Loss: 0.4448\n",
      "Epoch [134/200], Average Loss: 0.4550\n",
      "Epoch [135/200], Average Loss: 0.4400\n",
      "Epoch [136/200], Average Loss: 0.4137\n",
      "Epoch [137/200], Average Loss: 0.4648\n",
      "Epoch [138/200], Average Loss: 0.4557\n",
      "Epoch [139/200], Average Loss: 0.4460\n",
      "Epoch [140/200], Average Loss: 0.4838\n",
      "Epoch [141/200], Average Loss: 0.4048\n",
      "Epoch [142/200], Average Loss: 0.5240\n",
      "Epoch [143/200], Average Loss: 0.4514\n",
      "Epoch [144/200], Average Loss: 0.4380\n",
      "Epoch [145/200], Average Loss: 0.4153\n",
      "Epoch [146/200], Average Loss: 0.3871\n",
      "Epoch [147/200], Average Loss: 0.4458\n",
      "Epoch [148/200], Average Loss: 0.4275\n",
      "Epoch [149/200], Average Loss: 0.4098\n",
      "Epoch [150/200], Average Loss: 0.4264\n",
      "Epoch [151/200], Average Loss: 0.4914\n",
      "Epoch [152/200], Average Loss: 0.4169\n",
      "Epoch [153/200], Average Loss: 0.4220\n",
      "Epoch [154/200], Average Loss: 0.4684\n",
      "Epoch [155/200], Average Loss: 0.4430\n",
      "Epoch [156/200], Average Loss: 0.4836\n",
      "Epoch [157/200], Average Loss: 0.5172\n",
      "Epoch [158/200], Average Loss: 0.4637\n",
      "Epoch [159/200], Average Loss: 0.4211\n",
      "Epoch [160/200], Average Loss: 0.4476\n",
      "Epoch [161/200], Average Loss: 0.4067\n",
      "Epoch [162/200], Average Loss: 0.4089\n",
      "Epoch [163/200], Average Loss: 0.4075\n",
      "Epoch [164/200], Average Loss: 0.4343\n",
      "Epoch [165/200], Average Loss: 0.4333\n",
      "Epoch [166/200], Average Loss: 0.4044\n",
      "Epoch [167/200], Average Loss: 0.4160\n",
      "Epoch [168/200], Average Loss: 0.4236\n",
      "Epoch [169/200], Average Loss: 0.3823\n",
      "Epoch [170/200], Average Loss: 0.3724\n",
      "Epoch [171/200], Average Loss: 0.3836\n",
      "Epoch [172/200], Average Loss: 0.4330\n",
      "Epoch [173/200], Average Loss: 0.4524\n",
      "Epoch [174/200], Average Loss: 0.4344\n",
      "Epoch [175/200], Average Loss: 0.3882\n",
      "Epoch [176/200], Average Loss: 0.4374\n",
      "Epoch [177/200], Average Loss: 0.4441\n",
      "Epoch [178/200], Average Loss: 0.4437\n",
      "Epoch [179/200], Average Loss: 0.4769\n",
      "Epoch [180/200], Average Loss: 0.4235\n",
      "Epoch [181/200], Average Loss: 0.4335\n",
      "Epoch [182/200], Average Loss: 0.3960\n",
      "Epoch [183/200], Average Loss: 0.4116\n",
      "Epoch [184/200], Average Loss: 0.3758\n",
      "Epoch [185/200], Average Loss: 0.3998\n",
      "Epoch [186/200], Average Loss: 0.4169\n",
      "Epoch [187/200], Average Loss: 0.4178\n",
      "Epoch [188/200], Average Loss: 0.4022\n",
      "Epoch [189/200], Average Loss: 0.3878\n",
      "Epoch [190/200], Average Loss: 0.3991\n",
      "Epoch [191/200], Average Loss: 0.3913\n",
      "Epoch [192/200], Average Loss: 0.3744\n",
      "Epoch [193/200], Average Loss: 0.3614\n",
      "Epoch [194/200], Average Loss: 0.3637\n",
      "Epoch [195/200], Average Loss: 0.3440\n",
      "Epoch [196/200], Average Loss: 0.3531\n",
      "Epoch [197/200], Average Loss: 0.4243\n",
      "Epoch [198/200], Average Loss: 0.3600\n",
      "Epoch [199/200], Average Loss: 0.4105\n",
      "Epoch [200/200], Average Loss: 0.4193\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "VOCAB_SIZE = len(word2index)\n",
    "EMB_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_DROP = 0.5\n",
    "DEC_DROP = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "enc = Encoder(VOCAB_SIZE, EMB_DIM, HIDDEN_DIM, ENC_LAYERS, ENC_DROP).to(device)\n",
    "attn = LuongAttention(HIDDEN_DIM).to(device)\n",
    "dec = Decoder(VOCAB_SIZE, HIDDEN_DIM, DEC_LAYERS, DEC_DROP, attn).to(device)\n",
    "model = Seq2SeqAtt(enc, dec, device).to(device)\n",
    "print(model)\n",
    "\n",
    "# Create Dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "data_loader = DataLoader(machine_translate_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# define hypermeters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "num_epochs = 200\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, (source, target) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        source = source.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(source, target)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        target = target[1:].view(-1)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}\")\n",
    "    \n",
    "    # Print epoch statistics\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Average Loss: 0.1909\n",
      "Epoch [2/200], Average Loss: 0.1747\n",
      "Epoch [3/200], Average Loss: 0.1881\n",
      "Epoch [4/200], Average Loss: 0.1881\n",
      "Epoch [5/200], Average Loss: 0.1856\n",
      "Epoch [6/200], Average Loss: 0.2001\n",
      "Epoch [7/200], Average Loss: 0.2165\n",
      "Epoch [8/200], Average Loss: 0.1817\n",
      "Epoch [9/200], Average Loss: 0.1991\n",
      "Epoch [10/200], Average Loss: 0.1782\n",
      "Epoch [11/200], Average Loss: 0.1945\n",
      "Epoch [12/200], Average Loss: 0.2172\n",
      "Epoch [13/200], Average Loss: 0.1879\n",
      "Epoch [14/200], Average Loss: 0.2111\n",
      "Epoch [15/200], Average Loss: 0.1766\n",
      "Epoch [16/200], Average Loss: 0.1734\n",
      "Epoch [17/200], Average Loss: 0.1690\n",
      "Epoch [18/200], Average Loss: 0.1674\n",
      "Epoch [19/200], Average Loss: 0.2014\n",
      "Epoch [20/200], Average Loss: 0.1994\n",
      "Epoch [21/200], Average Loss: 0.1878\n",
      "Epoch [22/200], Average Loss: 0.1956\n",
      "Epoch [23/200], Average Loss: 0.2017\n",
      "Epoch [24/200], Average Loss: 0.1917\n",
      "Epoch [25/200], Average Loss: 0.1895\n",
      "Epoch [26/200], Average Loss: 0.1788\n",
      "Epoch [27/200], Average Loss: 0.1843\n",
      "Epoch [28/200], Average Loss: 0.1638\n",
      "Epoch [29/200], Average Loss: 0.1815\n",
      "Epoch [30/200], Average Loss: 0.1639\n",
      "Epoch [31/200], Average Loss: 0.1957\n",
      "Epoch [32/200], Average Loss: 0.1883\n",
      "Epoch [33/200], Average Loss: 0.1890\n",
      "Epoch [34/200], Average Loss: 0.1770\n",
      "Epoch [35/200], Average Loss: 0.1772\n",
      "Epoch [36/200], Average Loss: 0.1744\n",
      "Epoch [37/200], Average Loss: 0.1662\n",
      "Epoch [38/200], Average Loss: 0.1712\n",
      "Epoch [39/200], Average Loss: 0.1977\n",
      "Epoch [40/200], Average Loss: 0.1839\n",
      "Epoch [41/200], Average Loss: 0.1875\n",
      "Epoch [42/200], Average Loss: 0.1736\n",
      "Epoch [43/200], Average Loss: 0.1705\n",
      "Epoch [44/200], Average Loss: 0.1870\n",
      "Epoch [45/200], Average Loss: 0.1833\n",
      "Epoch [46/200], Average Loss: 0.1902\n",
      "Epoch [47/200], Average Loss: 0.1725\n",
      "Epoch [48/200], Average Loss: 0.1775\n",
      "Epoch [49/200], Average Loss: 0.1611\n",
      "Epoch [50/200], Average Loss: 0.1747\n",
      "Epoch [51/200], Average Loss: 0.1903\n",
      "Epoch [52/200], Average Loss: 0.1780\n",
      "Epoch [53/200], Average Loss: 0.1609\n",
      "Epoch [54/200], Average Loss: 0.1810\n",
      "Epoch [55/200], Average Loss: 0.1724\n",
      "Epoch [56/200], Average Loss: 0.1818\n",
      "Epoch [57/200], Average Loss: 0.1645\n",
      "Epoch [58/200], Average Loss: 0.1653\n",
      "Epoch [59/200], Average Loss: 0.1684\n",
      "Epoch [60/200], Average Loss: 0.1744\n",
      "Epoch [61/200], Average Loss: 0.1577\n",
      "Epoch [62/200], Average Loss: 0.1706\n",
      "Epoch [63/200], Average Loss: 0.1670\n",
      "Epoch [64/200], Average Loss: 0.1605\n",
      "Epoch [65/200], Average Loss: 0.1635\n",
      "Epoch [66/200], Average Loss: 0.1802\n",
      "Epoch [67/200], Average Loss: 0.1726\n",
      "Epoch [68/200], Average Loss: 0.1736\n",
      "Epoch [69/200], Average Loss: 0.1803\n",
      "Epoch [70/200], Average Loss: 0.1855\n",
      "Epoch [71/200], Average Loss: 0.1699\n",
      "Epoch [72/200], Average Loss: 0.1811\n",
      "Epoch [73/200], Average Loss: 0.1784\n",
      "Epoch [74/200], Average Loss: 0.1645\n",
      "Epoch [75/200], Average Loss: 0.1638\n",
      "Epoch [76/200], Average Loss: 0.1673\n",
      "Epoch [77/200], Average Loss: 0.1917\n",
      "Epoch [78/200], Average Loss: 0.1691\n",
      "Epoch [79/200], Average Loss: 0.1637\n",
      "Epoch [80/200], Average Loss: 0.1571\n",
      "Epoch [81/200], Average Loss: 0.1675\n",
      "Epoch [82/200], Average Loss: 0.1605\n",
      "Epoch [83/200], Average Loss: 0.1751\n",
      "Epoch [84/200], Average Loss: 0.1522\n",
      "Epoch [85/200], Average Loss: 0.1639\n",
      "Epoch [86/200], Average Loss: 0.1530\n",
      "Epoch [87/200], Average Loss: 0.1820\n",
      "Epoch [88/200], Average Loss: 0.1586\n",
      "Epoch [89/200], Average Loss: 0.1790\n",
      "Epoch [90/200], Average Loss: 0.1586\n",
      "Epoch [91/200], Average Loss: 0.1529\n",
      "Epoch [92/200], Average Loss: 0.1550\n",
      "Epoch [93/200], Average Loss: 0.1975\n",
      "Epoch [94/200], Average Loss: 0.1596\n",
      "Epoch [95/200], Average Loss: 0.1579\n",
      "Epoch [96/200], Average Loss: 0.1642\n",
      "Epoch [97/200], Average Loss: 0.1689\n",
      "Epoch [98/200], Average Loss: 0.1591\n",
      "Epoch [99/200], Average Loss: 0.1785\n",
      "Epoch [100/200], Average Loss: 0.1536\n",
      "Epoch [101/200], Average Loss: 0.1486\n",
      "Epoch [102/200], Average Loss: 0.1661\n",
      "Epoch [103/200], Average Loss: 0.1820\n",
      "Epoch [104/200], Average Loss: 0.1498\n",
      "Epoch [105/200], Average Loss: 0.1802\n",
      "Epoch [106/200], Average Loss: 0.1529\n",
      "Epoch [107/200], Average Loss: 0.1505\n",
      "Epoch [108/200], Average Loss: 0.1700\n",
      "Epoch [109/200], Average Loss: 0.1569\n",
      "Epoch [110/200], Average Loss: 0.1585\n",
      "Epoch [111/200], Average Loss: 0.1521\n",
      "Epoch [112/200], Average Loss: 0.1684\n",
      "Epoch [113/200], Average Loss: 0.1459\n",
      "Epoch [114/200], Average Loss: 0.1476\n",
      "Epoch [115/200], Average Loss: 0.1562\n",
      "Epoch [116/200], Average Loss: 0.1457\n",
      "Epoch [117/200], Average Loss: 0.1748\n",
      "Epoch [118/200], Average Loss: 0.1499\n",
      "Epoch [119/200], Average Loss: 0.1645\n",
      "Epoch [120/200], Average Loss: 0.1475\n",
      "Epoch [121/200], Average Loss: 0.1478\n",
      "Epoch [122/200], Average Loss: 0.1636\n",
      "Epoch [123/200], Average Loss: 0.1634\n",
      "Epoch [124/200], Average Loss: 0.1631\n",
      "Epoch [125/200], Average Loss: 0.1538\n",
      "Epoch [126/200], Average Loss: 0.1715\n",
      "Epoch [127/200], Average Loss: 0.1429\n",
      "Epoch [128/200], Average Loss: 0.1580\n",
      "Epoch [129/200], Average Loss: 0.1585\n",
      "Epoch [130/200], Average Loss: 0.1599\n",
      "Epoch [131/200], Average Loss: 0.1619\n",
      "Epoch [132/200], Average Loss: 0.1523\n",
      "Epoch [133/200], Average Loss: 0.1535\n",
      "Epoch [134/200], Average Loss: 0.1469\n",
      "Epoch [135/200], Average Loss: 0.1625\n",
      "Epoch [136/200], Average Loss: 0.1489\n",
      "Epoch [137/200], Average Loss: 0.1641\n",
      "Epoch [138/200], Average Loss: 0.1478\n",
      "Epoch [139/200], Average Loss: 0.1731\n",
      "Epoch [140/200], Average Loss: 0.1541\n",
      "Epoch [141/200], Average Loss: 0.1498\n",
      "Epoch [142/200], Average Loss: 0.1566\n",
      "Epoch [143/200], Average Loss: 0.1543\n",
      "Epoch [144/200], Average Loss: 0.1431\n",
      "Epoch [145/200], Average Loss: 0.1555\n",
      "Epoch [146/200], Average Loss: 0.1555\n",
      "Epoch [147/200], Average Loss: 0.1561\n",
      "Epoch [148/200], Average Loss: 0.1392\n",
      "Epoch [149/200], Average Loss: 0.1757\n",
      "Epoch [150/200], Average Loss: 0.1559\n",
      "Epoch [151/200], Average Loss: 0.1391\n",
      "Epoch [152/200], Average Loss: 0.1437\n",
      "Epoch [153/200], Average Loss: 0.1511\n",
      "Epoch [154/200], Average Loss: 0.1632\n",
      "Epoch [155/200], Average Loss: 0.1858\n",
      "Epoch [156/200], Average Loss: 0.1462\n",
      "Epoch [157/200], Average Loss: 0.1460\n",
      "Epoch [158/200], Average Loss: 0.1656\n",
      "Epoch [159/200], Average Loss: 0.1406\n",
      "Epoch [160/200], Average Loss: 0.1416\n",
      "Epoch [161/200], Average Loss: 0.1584\n",
      "Epoch [162/200], Average Loss: 0.1480\n",
      "Epoch [163/200], Average Loss: 0.1443\n",
      "Epoch [164/200], Average Loss: 0.1479\n",
      "Epoch [165/200], Average Loss: 0.1832\n",
      "Epoch [166/200], Average Loss: 0.1457\n",
      "Epoch [167/200], Average Loss: 0.1429\n",
      "Epoch [168/200], Average Loss: 0.1446\n",
      "Epoch [169/200], Average Loss: 0.1354\n",
      "Epoch [170/200], Average Loss: 0.1470\n",
      "Epoch [171/200], Average Loss: 0.1496\n",
      "Epoch [172/200], Average Loss: 0.1554\n",
      "Epoch [173/200], Average Loss: 0.1503\n",
      "Epoch [174/200], Average Loss: 0.1408\n",
      "Epoch [175/200], Average Loss: 0.1357\n",
      "Epoch [176/200], Average Loss: 0.1375\n",
      "Epoch [177/200], Average Loss: 0.1349\n",
      "Epoch [178/200], Average Loss: 0.1648\n",
      "Epoch [179/200], Average Loss: 0.1386\n",
      "Epoch [180/200], Average Loss: 0.1353\n",
      "Epoch [181/200], Average Loss: 0.1468\n",
      "Epoch [182/200], Average Loss: 0.1523\n",
      "Epoch [183/200], Average Loss: 0.1439\n",
      "Epoch [184/200], Average Loss: 0.1421\n",
      "Epoch [185/200], Average Loss: 0.1503\n",
      "Epoch [186/200], Average Loss: 0.1329\n",
      "Epoch [187/200], Average Loss: 0.1355\n",
      "Epoch [188/200], Average Loss: 0.1519\n",
      "Epoch [189/200], Average Loss: 0.1373\n",
      "Epoch [190/200], Average Loss: 0.1427\n",
      "Epoch [191/200], Average Loss: 0.1540\n",
      "Epoch [192/200], Average Loss: 0.1337\n",
      "Epoch [193/200], Average Loss: 0.1462\n",
      "Epoch [194/200], Average Loss: 0.1369\n",
      "Epoch [195/200], Average Loss: 0.1369\n",
      "Epoch [196/200], Average Loss: 0.1452\n",
      "Epoch [197/200], Average Loss: 0.1362\n",
      "Epoch [198/200], Average Loss: 0.1363\n",
      "Epoch [199/200], Average Loss: 0.1283\n",
      "Epoch [200/200], Average Loss: 0.1349\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, (source, target) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        source = source.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(source, target)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        target = target[1:].view(-1)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}\")\n",
    "    \n",
    "    # Print epoch statistics\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequence, max_len, word2index):\n",
    "    if len(sequence) > max_len:\n",
    "        return sequence[:max_len]\n",
    "    else:\n",
    "        return sequence + [word2index[\"<pad>\"]] * (max_len - len(sequence))\n",
    "\n",
    "def translate(model, src_sentence, word2index, device, max_length=50):\n",
    "\n",
    "    model.eval()\n",
    "    # Convert sentence to indices\n",
    "    src_indexes = [word2index.get(token, word2index[\"<unk>\"]) for token in src_sentence]\n",
    "    src_indexes = [word2index[\"<sos>\"]] + src_indexes + [word2index[\"<eos>\"]]\n",
    "    src_indexes = pad_sequence(src_indexes, max_length, word2index)\n",
    "\n",
    "    # Convert to tensor\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get encoder outputs\n",
    "        encoder_outputs, hidden, cell = model.encoder(src_tensor)\n",
    "        \n",
    "        # Initialize decoder input with <sos> token\n",
    "        decoder_input = torch.LongTensor([word2index[\"<sos>\"]]).to(device)\n",
    "        \n",
    "        # Store all decoder outputs\n",
    "        decoded_words = []\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Run decoder for one step\n",
    "            decoder_output, hidden = model.decoder(decoder_input, hidden, encoder_outputs)\n",
    "            \n",
    "            # Get the most likely word\n",
    "            topv, topi = decoder_output.squeeze().data.topk(1)\n",
    "            decoded_token = topi.item()\n",
    "            \n",
    "            # Add the token to results\n",
    "            if decoded_token == word2index[\"<eos>\"]:\n",
    "                break\n",
    "            elif decoded_token == word2index[\"<pad>\"]:\n",
    "                continue\n",
    "            else:\n",
    "                # Convert index back to word\n",
    "                decoded_words.append(\n",
    "                    next(word for word, index in word2index.items() \n",
    "                        if index == decoded_token)\n",
    "                )\n",
    "            \n",
    "            # Next input is the decoded token\n",
    "            decoder_input = torch.LongTensor([decoded_token]).to(device)\n",
    "    \n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tôi đang đi đến cửa hàng.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "translated = translate(\n",
    "    model=model,\n",
    "    src_sentence=\"I am going to the store.\".split(),\n",
    "    word2index=word2index,\n",
    "    device=device,\n",
    "    max_length=50\n",
    ")\n",
    "print(\" \".join(translated))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
