{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence Model Overview\n",
    "\n",
    "## Understanding Concept and Definition\n",
    "\n",
    "A Sequence-to-Sequence (Seq2Seq) model is a type of neural network architecture designed to transform a given sequence of elements, such as words in a sentence, into another sequence. This model is widely used in various natural language processing (NLP) tasks, including machine translation, text summarization, and conversational agents.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Encoder**: The encoder processes the input sequence and compresses the information into a fixed-size context vector. It typically consists of recurrent neural networks (RNNs), long short-term memory networks (LSTMs), or gated recurrent units (GRUs).\n",
    "\n",
    "2. **Decoder**: The decoder takes the context vector from the encoder and generates the output sequence. It also uses RNNs, LSTMs, or GRUs and often incorporates attention mechanisms to focus on different parts of the input sequence during the generation process.\n",
    "\n",
    "3. **Attention Mechanism**: An optional component that allows the decoder to selectively focus on different parts of the input sequence, improving the model's performance on longer sequences and complex tasks.\n",
    "\n",
    "### Applications\n",
    "\n",
    "- **Machine Translation**: Translating text from one language to another.\n",
    "- **Text Summarization**: Condensing long documents into shorter summaries.\n",
    "- **Conversational Agents**: Generating responses in dialogue systems.\n",
    "- **Speech Recognition**: Converting spoken language into text.\n",
    "\n",
    "Seq2Seq models have revolutionized NLP by providing a flexible and powerful framework for handling various sequence transformation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Seq2Seq without Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        # embedded shape: (batch_size, seq_length, embedding_size)\n",
    "        \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded, (h0, c0))\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_size, hidden_size, num_layers, dropout=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape: (batch_size, 1)\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        # embedded shape: (batch_size, 1, embedding_size)\n",
    "        \n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc(output)\n",
    "        \n",
    "        # prediction shape: (batch_size, 1, output_size)\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src shape: (batch_size, src_seq_length)\n",
    "        # trg shape: (batch_size, trg_seq_length)\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # encoder_outputs shape: (batch_size, src_seq_length, hidden_size)\n",
    "        # hidden shape: (num_layers, batch_size, hidden_size)\n",
    "        # cell shape: (num_layers, batch_size, hidden_size)\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # first input to the decoder is the <sos> token\n",
    "        _input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(_input, hidden, cell)\n",
    "            \n",
    "            # output shape: (batch_size, 1, output_size)\n",
    "            outputs[:, t] = output.squeeze(1)\n",
    "            \n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            \n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(2).squeeze(1)\n",
    "            \n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            _input = trg[:, t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'Hello, how are you?', 'target': 'Xin chào, bạn khỏe không?'}\n",
      "{'source': 'What is your name?', 'target': 'Tên bạn là gì?'}\n",
      "{'source': 'I love programming.', 'target': 'Tôi yêu lập trình.'}\n",
      "{'source': 'The weather is nice today.', 'target': 'Thời tiết hôm nay đẹp.'}\n",
      "{'source': 'She is reading a book.', 'target': 'Cô ấy đang đọc sách.'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Source and target sentences for machine translation training\n",
    "source_sentences = [\n",
    "    \"Hello, how are you?\", \"What is your name?\", \"I love programming.\",\n",
    "    \"The weather is nice today.\", \"She is reading a book.\", \"Where is the nearest bus stop?\",\n",
    "    \"Can you help me?\", \"This is a beautiful place.\", \"I am learning a new language.\",\n",
    "    \"We will go to the park tomorrow.\", \"I need a cup of coffee.\", \"He is my best friend.\",\n",
    "    \"How much does this cost?\", \"The sun is shining brightly.\", \"She enjoys listening to music.\",\n",
    "    \"Do you speak English?\", \"This food tastes amazing.\", \"Where do you live?\",\n",
    "    \"Let's go to the cinema.\", \"I'm feeling very happy today.\", \"He is a great teacher.\",\n",
    "    \"My favorite color is blue.\", \"Please turn off the lights.\", \"I have a pet dog.\",\n",
    "    \"The train arrives at 10 AM.\", \"We should visit the museum.\", \"She wants to buy a new dress.\",\n",
    "    \"It's time to go to bed.\", \"Could you repeat that, please?\", \"I'm studying artificial intelligence.\"\n",
    "]\n",
    "\n",
    "target_sentences = [\n",
    "    \"Xin chào, bạn khỏe không?\", \"Tên bạn là gì?\", \"Tôi yêu lập trình.\",\n",
    "    \"Thời tiết hôm nay đẹp.\", \"Cô ấy đang đọc sách.\", \"Trạm xe buýt gần nhất ở đâu?\",\n",
    "    \"Bạn có thể giúp tôi không?\", \"Đây là một nơi đẹp.\", \"Tôi đang học một ngôn ngữ mới.\",\n",
    "    \"Chúng ta sẽ đi công viên vào ngày mai.\", \"Tôi cần một tách cà phê.\", \"Anh ấy là bạn thân nhất của tôi.\",\n",
    "    \"Cái này giá bao nhiêu?\", \"Mặt trời đang chiếu sáng rực rỡ.\", \"Cô ấy thích nghe nhạc.\",\n",
    "    \"Bạn có nói tiếng Anh không?\", \"Món ăn này có vị tuyệt vời.\", \"Bạn sống ở đâu?\",\n",
    "    \"Hãy đi đến rạp chiếu phim.\", \"Hôm nay tôi cảm thấy rất vui.\", \"Anh ấy là một giáo viên tuyệt vời.\",\n",
    "    \"Màu yêu thích của tôi là màu xanh.\", \"Vui lòng tắt đèn.\", \"Tôi có một con chó cưng.\",\n",
    "    \"Tàu sẽ đến lúc 10 giờ sáng.\", \"Chúng ta nên đi thăm bảo tàng.\", \"Cô ấy muốn mua một chiếc váy mới.\",\n",
    "    \"Đã đến lúc đi ngủ rồi.\", \"Bạn có thể nhắc lại không?\", \"Tôi đang học trí tuệ nhân tạo.\"\n",
    "]\n",
    "\n",
    "# Create random translation data\n",
    "translation_data = [{\"source\": src, \"target\": tgt} for src, tgt in zip(source_sentences, target_sentences)]\n",
    "\n",
    "# Save into a list\n",
    "train_data = list(translation_data)\n",
    "\n",
    "# Print example data\n",
    "for example in train_data[:5]:  # Show first 5 examples\n",
    "    print(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data \n",
    "word2index = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "\n",
    "# add source_sentences & target_sentences to word2index\n",
    "for example in train_data:\n",
    "    for word in example[\"source\"].split():\n",
    "        if word not in word2index:\n",
    "            word2index[word] = len(word2index)\n",
    "    for word in example[\"target\"].split():\n",
    "        if word not in word2index:\n",
    "            word2index[word] = len(word2index)\n",
    "\n",
    "index2word = {index: word for word, index in word2index.items()}\n",
    "vocab_size = len(word2index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineTranslateDataset:\n",
    "\n",
    "    def __init__(self, data, word2index, max_length=50):\n",
    "        self.data = data\n",
    "        self.word2index = word2index\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def pad_sequence(self, sequence, max_len):\n",
    "        # Truncate if sequence is longer than max_len\n",
    "        if len(sequence) > max_len:\n",
    "            return sequence[:max_len]\n",
    "        # Pad with <pad> token if sequence is shorter\n",
    "        else:\n",
    "            return sequence + [self.word2index[\"<pad>\"]] * (max_len - len(sequence))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        src = self.data[index][\"source\"]\n",
    "        trg = self.data[index][\"target\"]\n",
    "        \n",
    "        # Convert words to indices\n",
    "        src_indexes = [self.word2index.get(word, self.word2index[\"<unk>\"]) \n",
    "                      for word in src.split()]\n",
    "        trg_indexes = [self.word2index.get(word, self.word2index[\"<unk>\"]) \n",
    "                      for word in trg.split()]\n",
    "        \n",
    "        # Add <sos> and <eos> tokens\n",
    "        src_indexes = [self.word2index[\"<sos>\"]] + src_indexes + [self.word2index[\"<eos>\"]]\n",
    "        trg_indexes = [self.word2index[\"<sos>\"]] + trg_indexes + [self.word2index[\"<eos>\"]]\n",
    "        \n",
    "        # Pad sequences\n",
    "        src_indexes = self.pad_sequence(src_indexes, self.max_length)\n",
    "        trg_indexes = self.pad_sequence(trg_indexes, self.max_length)\n",
    "        \n",
    "        return (torch.tensor(src_indexes, dtype=torch.long), \n",
    "                torch.tensor(trg_indexes, dtype=torch.long))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 4, 5, 6, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]),\n",
       " tensor([ 1,  8,  9, 10, 11, 12,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_translate_dataset =  MachineTranslateDataset(translation_data, word2index)\n",
    "machine_translate_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(233, 256)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(233, 256)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "    (fc): Linear(in_features=512, out_features=233, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size_encoder = vocab_size\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 8\n",
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "\n",
    "# Initialize dataloader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=machine_translate_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "# Initialize model\n",
    "model = Seq2Seq(encoder=Encoder(vocab_size, embedding_size, hidden_size, num_layers),\n",
    "                decoder=Decoder(vocab_size, embedding_size, hidden_size, num_layers),\n",
    "                device=device).to(device)\n",
    "print(model)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Average Loss: 0.1345\n",
      "Epoch [2/100], Average Loss: 0.1535\n",
      "Epoch [3/100], Average Loss: 0.1668\n",
      "Epoch [4/100], Average Loss: 0.2068\n",
      "Epoch [5/100], Average Loss: 0.1242\n",
      "Epoch [6/100], Average Loss: 0.1790\n",
      "Epoch [7/100], Average Loss: 0.2777\n",
      "Epoch [8/100], Average Loss: 0.3051\n",
      "Epoch [9/100], Average Loss: 0.1795\n",
      "Epoch [10/100], Average Loss: 0.1250\n",
      "Epoch [11/100], Average Loss: 0.1986\n",
      "Epoch [12/100], Average Loss: 0.1575\n",
      "Epoch [13/100], Average Loss: 0.1480\n",
      "Epoch [14/100], Average Loss: 0.1427\n",
      "Epoch [15/100], Average Loss: 0.1939\n",
      "Epoch [16/100], Average Loss: 0.1271\n",
      "Epoch [17/100], Average Loss: 0.1676\n",
      "Epoch [18/100], Average Loss: 0.2059\n",
      "Epoch [19/100], Average Loss: 0.2145\n",
      "Epoch [20/100], Average Loss: 0.1542\n",
      "Epoch [21/100], Average Loss: 0.1271\n",
      "Epoch [22/100], Average Loss: 0.2323\n",
      "Epoch [23/100], Average Loss: 0.1123\n",
      "Epoch [24/100], Average Loss: 0.0937\n",
      "Epoch [25/100], Average Loss: 0.1188\n",
      "Epoch [26/100], Average Loss: 0.2041\n",
      "Epoch [27/100], Average Loss: 0.1781\n",
      "Epoch [28/100], Average Loss: 0.1530\n",
      "Epoch [29/100], Average Loss: 0.1284\n",
      "Epoch [30/100], Average Loss: 0.1278\n",
      "Epoch [31/100], Average Loss: 0.1312\n",
      "Epoch [32/100], Average Loss: 0.1085\n",
      "Epoch [33/100], Average Loss: 0.1024\n",
      "Epoch [34/100], Average Loss: 0.1529\n",
      "Epoch [35/100], Average Loss: 0.0944\n",
      "Epoch [36/100], Average Loss: 0.1122\n",
      "Epoch [37/100], Average Loss: 0.1625\n",
      "Epoch [38/100], Average Loss: 0.1918\n",
      "Epoch [39/100], Average Loss: 0.1956\n",
      "Epoch [40/100], Average Loss: 0.1777\n",
      "Epoch [41/100], Average Loss: 0.2439\n",
      "Epoch [42/100], Average Loss: 0.1840\n",
      "Epoch [43/100], Average Loss: 0.0985\n",
      "Epoch [44/100], Average Loss: 0.1297\n",
      "Epoch [45/100], Average Loss: 0.1121\n",
      "Epoch [46/100], Average Loss: 0.1510\n",
      "Epoch [47/100], Average Loss: 0.1286\n",
      "Epoch [48/100], Average Loss: 0.1182\n",
      "Epoch [49/100], Average Loss: 0.1161\n",
      "Epoch [50/100], Average Loss: 0.2153\n",
      "Epoch [51/100], Average Loss: 0.1018\n",
      "Epoch [52/100], Average Loss: 0.1398\n",
      "Epoch [53/100], Average Loss: 0.0894\n",
      "Epoch [54/100], Average Loss: 0.0975\n",
      "Epoch [55/100], Average Loss: 0.0719\n",
      "Epoch [56/100], Average Loss: 0.0958\n",
      "Epoch [57/100], Average Loss: 0.0706\n",
      "Epoch [58/100], Average Loss: 0.0921\n",
      "Epoch [59/100], Average Loss: 0.0956\n",
      "Epoch [60/100], Average Loss: 0.1212\n",
      "Epoch [61/100], Average Loss: 0.0856\n",
      "Epoch [62/100], Average Loss: 0.0813\n",
      "Epoch [63/100], Average Loss: 0.0728\n",
      "Epoch [64/100], Average Loss: 0.0784\n",
      "Epoch [65/100], Average Loss: 0.0851\n",
      "Epoch [66/100], Average Loss: 0.0820\n",
      "Epoch [67/100], Average Loss: 0.0637\n",
      "Epoch [68/100], Average Loss: 0.0795\n",
      "Epoch [69/100], Average Loss: 0.0699\n",
      "Epoch [70/100], Average Loss: 0.1287\n",
      "Epoch [71/100], Average Loss: 0.0668\n",
      "Epoch [72/100], Average Loss: 0.0734\n",
      "Epoch [73/100], Average Loss: 0.0685\n",
      "Epoch [74/100], Average Loss: 0.0696\n",
      "Epoch [75/100], Average Loss: 0.0693\n",
      "Epoch [76/100], Average Loss: 0.0617\n",
      "Epoch [77/100], Average Loss: 0.0976\n",
      "Epoch [78/100], Average Loss: 0.0648\n",
      "Epoch [79/100], Average Loss: 0.0601\n",
      "Epoch [80/100], Average Loss: 0.0444\n",
      "Epoch [81/100], Average Loss: 0.0558\n",
      "Epoch [82/100], Average Loss: 0.0497\n",
      "Epoch [83/100], Average Loss: 0.0492\n",
      "Epoch [84/100], Average Loss: 0.0490\n",
      "Epoch [85/100], Average Loss: 0.0417\n",
      "Epoch [86/100], Average Loss: 0.0343\n",
      "Epoch [87/100], Average Loss: 0.0399\n",
      "Epoch [88/100], Average Loss: 0.0377\n",
      "Epoch [89/100], Average Loss: 0.0485\n",
      "Epoch [90/100], Average Loss: 0.0403\n",
      "Epoch [91/100], Average Loss: 0.0569\n",
      "Epoch [92/100], Average Loss: 0.0317\n",
      "Epoch [93/100], Average Loss: 0.0284\n",
      "Epoch [94/100], Average Loss: 0.0421\n",
      "Epoch [95/100], Average Loss: 0.0439\n",
      "Epoch [96/100], Average Loss: 0.0332\n",
      "Epoch [97/100], Average Loss: 0.0348\n",
      "Epoch [98/100], Average Loss: 0.0258\n",
      "Epoch [99/100], Average Loss: 0.0231\n",
      "Epoch [100/100], Average Loss: 0.0221\n"
     ]
    }
   ],
   "source": [
    "#  Training loop\n",
    "model.train()\n",
    "total_steps = len(data_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, (src, trg) in enumerate(data_loader):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "       # Calculate loss\n",
    "        # output shape: (batch_size, seq_len, vocab_size)\n",
    "        # target shape: (batch_size, seq_len)\n",
    "        output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "        target = trg[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        # Print progress\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                    f'Step [{i+1}/{total_steps}], '\n",
    "                    f'Loss: {loss.item():.4f}')\n",
    "            \n",
    "    # Print epoch statistics\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequence, max_len, word2index):\n",
    "    if len(sequence) > max_len:\n",
    "        return sequence[:max_len]\n",
    "    else:\n",
    "        return sequence + [word2index[\"<pad>\"]] * (max_len - len(sequence))\n",
    "\n",
    "def translate(model, src_sentence, word2index, device, max_length=50):\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert sentence to indices\n",
    "    src_indexes = [word2index.get(token, word2index[\"<unk>\"]) for token in src_sentence]\n",
    "    src_indexes = [word2index[\"<sos>\"]] + src_indexes + [word2index[\"<eos>\"]]\n",
    "    src_indexes = pad_sequence(src_indexes, max_length, word2index)\n",
    "    \n",
    "    # Create source tensor: shape should be [batch_size=1, seq_len]\n",
    "    src_tensor = torch.LongTensor([src_indexes]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get encoder outputs\n",
    "        encoder_outputs, hidden, cell = model.encoder(src_tensor)\n",
    "        \n",
    "        # Initialize decoder input with <sos> token\n",
    "        decoder_input = torch.LongTensor([word2index[\"<sos>\"]]).to(device)\n",
    "        \n",
    "        # Store all decoder outputs\n",
    "        decoded_words = []\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Run decoder for one step\n",
    "            decoder_output, hidden, cell = model.decoder(decoder_input, hidden, cell)\n",
    "            \n",
    "            # Get the most likely word\n",
    "            topv, topi = decoder_output.squeeze().data.topk(1)\n",
    "            decoded_token = topi.item()\n",
    "            \n",
    "            # Add the token to results\n",
    "            if decoded_token == word2index[\"<eos>\"]:\n",
    "                break\n",
    "            elif decoded_token == word2index[\"<pad>\"]:\n",
    "                continue\n",
    "            else:\n",
    "                # Convert index back to word\n",
    "                decoded_words.append(\n",
    "                    next(word for word, index in word2index.items() \n",
    "                        if index == decoded_token)\n",
    "                )\n",
    "            \n",
    "            # Next input is the decoded token\n",
    "            decoder_input = torch.LongTensor([decoded_token]).to(device)\n",
    "    \n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and vocab\n",
    "import json\n",
    "import os\n",
    "model_dir = \"seq2se_model\"\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "def save_model():\n",
    "    torch.save(model.state_dict(), f\"{model_dir}/seq2seq.pth\")\n",
    "    with open(f\"{model_dir}/word2index.json\", \"w\") as f:\n",
    "        json.dump(word2index, f)\n",
    "\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thời tiết hôm nay đẹp.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "translated = translate(\n",
    "    model=model,\n",
    "    src_sentence=\"The weather is nice today.\".split(),\n",
    "    word2index=word2index,\n",
    "    device=device,\n",
    "    max_length=50\n",
    ")\n",
    "print(\" \".join(translated))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
